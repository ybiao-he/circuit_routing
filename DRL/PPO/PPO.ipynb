{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "first-thought",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE STANDARD VARIABLES\n",
      "Model Summary: simple_actor_critic\n",
      "Model: \"simple_actor_critic\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_actor_critic1 (Dense) (None, 32)                160       \n",
      "_________________________________________________________________\n",
      "simple_actor_critic2 (Dense) (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "simple_actor_critic_output ( (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 1,348\n",
      "Trainable params: 1,348\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"simple_actor_critic\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_actor_critic1 (Dense) (None, 32)                160       \n",
      "_________________________________________________________________\n",
      "simple_actor_critic2 (Dense) (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "simple_actor_critic_output ( (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,249\n",
      "Trainable params: 1,249\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "nan\n",
      "{'vec_obses': array([[ 12.,  13.,  -9.,   2.],\n",
      "       [ 12.,  12.,  -9.,   1.],\n",
      "       [ 12.,  11.,  -9.,   0.],\n",
      "       ...,\n",
      "       [ 17.,   0.,  -4., -11.],\n",
      "       [ 17.,   0.,  -4., -11.],\n",
      "       [ 17.,   0.,  -4., -11.]], dtype=float32), 'vis_obses': None, 'rews': array([-0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.1, -0.1, -0.1, -0.1, -0.1, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.1,\n",
      "       -0.1, -0.1, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.1, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.1, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.1, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "       -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5],\n",
      "      dtype=float32), 'dones': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False]), 'actions': array([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2,\n",
      "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3], dtype=int32), 'logp': array([-0.0163548 , -0.01781685, -0.01927058, -0.0209752 , -0.02210059,\n",
      "       -0.02166848, -0.02175737, -0.02130714, -0.02130714, -0.02130714,\n",
      "       -0.02130714, -0.02130714, -0.02130714, -0.02130714, -0.02130714,\n",
      "       -0.02130714, -0.02130714, -0.02130714, -0.02130714, -0.02130714,\n",
      "       -0.02130714, -0.02130714, -0.02130714, -0.02130714, -0.02130714,\n",
      "       -0.02130714, -0.02130714, -0.02130714, -0.02130714, -0.02130714,\n",
      "       -0.02130714, -0.02130714, -0.02130714, -0.02130714, -0.02130714,\n",
      "       -5.523444  , -0.02130714, -0.02130714, -0.02130714, -0.02130714,\n",
      "       -0.02130714, -0.02130714, -0.02130714, -0.02130714, -0.02130714,\n",
      "       -0.02130714, -0.02130714, -0.02130714, -0.02130714, -0.02130714,\n",
      "       -0.02130714, -0.02130714, -0.02130714, -0.02130714, -0.02130714,\n",
      "       -0.02130714, -0.02130714, -0.02130714, -0.02130714, -0.02130714,\n",
      "       -0.02130714, -0.02130714, -0.02130714, -0.02130714, -0.02130714,\n",
      "       -0.02130714, -0.02130714, -0.02130714, -0.02130714, -0.02130714,\n",
      "       -0.02130714, -0.02130714, -0.02130714, -5.523444  , -0.02130714,\n",
      "       -0.02130714, -0.02130714, -0.02130714, -0.02130714, -0.02130714,\n",
      "       -0.02130714, -0.02130714, -0.02130714, -0.02130714, -0.02130714,\n",
      "       -0.02130714, -0.02130714, -0.02130714, -0.02130714, -0.02130714,\n",
      "       -0.02130714, -0.02130714, -0.02130714, -4.451464  , -0.02215481,\n",
      "       -0.01987215, -0.01880204, -0.01668534, -0.0134673 , -0.0134673 ,\n",
      "       -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 ,\n",
      "       -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 ,\n",
      "       -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 ,\n",
      "       -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 ,\n",
      "       -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 ,\n",
      "       -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 ,\n",
      "       -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 ,\n",
      "       -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 ,\n",
      "       -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 ,\n",
      "       -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 ,\n",
      "       -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 ,\n",
      "       -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 ,\n",
      "       -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 ,\n",
      "       -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 ,\n",
      "       -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 ,\n",
      "       -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 ,\n",
      "       -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 ,\n",
      "       -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 ,\n",
      "       -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 ,\n",
      "       -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 ,\n",
      "       -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 ,\n",
      "       -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 ,\n",
      "       -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 ,\n",
      "       -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 ,\n",
      "       -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 ,\n",
      "       -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 ,\n",
      "       -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 ,\n",
      "       -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 , -0.0134673 ,\n",
      "       -0.0134673 , -4.9192405 , -0.01276548, -0.00920162, -0.00605463,\n",
      "       -0.00605463, -0.00605463, -0.00605463, -0.00605463, -0.00605463,\n",
      "       -0.00605463, -0.00605463, -0.00605463, -0.00605463, -0.00605463,\n",
      "       -0.00605463, -0.00605463, -0.00605463, -0.00605463, -0.00605463,\n",
      "       -0.00605463, -0.00605463, -0.00605463, -0.00605463, -0.00605463,\n",
      "       -0.00605463, -0.00605463, -0.00605463, -0.00605463, -0.00605463,\n",
      "       -0.00605463, -0.00605463, -0.00605463, -0.00605463, -0.00605463,\n",
      "       -0.00605463, -0.00605463, -0.00605463, -0.00605463, -0.00605463,\n",
      "       -0.00605463, -0.00605463, -0.00605463, -0.00605463, -0.00605463,\n",
      "       -0.00605463, -0.00605463, -0.00605463, -0.00605463, -0.00605463,\n",
      "       -0.00605463, -0.00605463, -0.00605463, -0.00605463, -0.00605463,\n",
      "       -0.00605463, -0.00605463, -0.00605463, -0.00605463, -0.00605463,\n",
      "       -0.00605463, -0.00605463, -0.00605463, -0.00605463, -0.00605463,\n",
      "       -0.00605463, -0.00605463, -0.00605463, -0.00605463, -0.00605463,\n",
      "       -0.00605463, -0.00605463, -0.00605463, -0.00605463, -0.00605463,\n",
      "       -0.00605463, -0.00605463, -0.00605463, -0.00605463, -0.00605463,\n",
      "       -0.00605463, -0.00605463, -0.00605463, -0.00605463, -0.00605463,\n",
      "       -0.00605463, -0.00605463, -0.00605463, -0.00605463, -0.00605463,\n",
      "       -0.00605463, -0.00605463, -0.00605463, -0.00605463, -0.00605463,\n",
      "       -0.00605463, -0.00605463, -0.00605463, -0.00605463, -0.00605463,\n",
      "       -0.00605463, -0.00605463, -0.00605463, -0.00605463, -0.00605463,\n",
      "       -0.00605463, -0.00605463, -0.00605463, -0.00605463, -0.00605463,\n",
      "       -0.00605463, -0.00605463, -0.00605463, -0.00605463, -0.00605463,\n",
      "       -0.00605463, -0.00605463, -0.00605463, -0.00605463, -0.00605463,\n",
      "       -0.00605463, -0.00605463, -0.00605463, -0.00605463, -0.00605463,\n",
      "       -0.00605463, -0.00605463, -0.00605463, -0.00605463, -0.00605463,\n",
      "       -0.00605463, -0.00605463, -0.00605463, -0.00605463, -0.00605463,\n",
      "       -0.00605463, -0.00605463, -0.00605463, -0.00605463, -0.00605463,\n",
      "       -0.00605463, -0.00605463, -0.00605463, -0.00605463, -0.00605463,\n",
      "       -0.00605463, -0.00605463, -0.00605463, -0.00605463, -0.00605463,\n",
      "       -0.00605463, -0.00605463, -0.00605463, -0.00605463, -0.00605463,\n",
      "       -0.00605463, -0.00605463, -0.00605463, -0.00605463, -0.00605463,\n",
      "       -0.00605463, -0.00605463, -0.00605463, -0.00605463, -0.00605463,\n",
      "       -0.00605463, -0.00605463, -0.00605463, -0.00605463, -0.00605463,\n",
      "       -0.00605463, -0.00605463, -0.00605463, -0.00605463, -0.00605463,\n",
      "       -0.00605463, -0.00605463, -0.00605463, -0.00605463, -0.00605463,\n",
      "       -0.00605463, -0.00605463, -0.00605463, -0.00605463, -0.00605463,\n",
      "       -0.00605463, -0.00605463, -0.00605463, -0.00605463, -0.00605463,\n",
      "       -0.00605463, -0.00605463, -0.00605463, -0.00605463, -0.00605463,\n",
      "       -0.00605463, -0.00605463, -0.00605463, -0.00605463, -0.00605463,\n",
      "       -0.00605463, -0.00605463, -0.00605463, -0.00605463, -0.00605463,\n",
      "       -0.00605463, -0.00605463, -0.00605463, -0.00605463, -0.00605463,\n",
      "       -0.00605463, -0.00605463, -0.00605463, -0.00605463, -0.00605463,\n",
      "       -0.00605463, -0.00605463, -0.00605463, -0.00605463, -0.00605463,\n",
      "       -0.00605463, -0.00605463, -0.00605463, -0.00605463, -0.00605463,\n",
      "       -5.3935304 , -0.00639772, -0.00639772, -0.00639772, -0.00639772,\n",
      "       -0.00639772, -0.00639772, -0.00639772, -0.00639772, -0.00639772,\n",
      "       -0.00639772, -0.00639772, -0.00639772, -0.00639772, -0.00639772,\n",
      "       -0.00639772, -0.00639772, -0.00639772, -0.00639772, -0.00639772,\n",
      "       -0.00639772, -0.00639772, -0.00639772, -0.00639772, -0.00639772,\n",
      "       -0.00639772, -0.00639772, -0.00639772, -0.00639772, -0.00639772,\n",
      "       -0.00639772, -0.00639772, -0.00639772, -0.00639772, -0.00639772,\n",
      "       -0.00639772, -0.00639772, -5.1591554 , -0.00739918, -0.00739918,\n",
      "       -0.00739918, -0.00739918, -0.00739918, -0.00739918, -0.00739918,\n",
      "       -0.00739918, -0.00739918, -0.00739918, -0.00739918, -0.00739918,\n",
      "       -0.00739918, -0.00739918, -0.00739918, -0.00739918, -0.00739918,\n",
      "       -0.00739918, -0.00739918, -0.00739918, -0.00739918, -0.00739918,\n",
      "       -0.00739918, -0.00739918, -0.00739918, -0.00739918, -0.00739918,\n",
      "       -0.00739918, -0.00739918, -0.00739918, -0.00739918, -0.00739918,\n",
      "       -0.00739918, -0.00739918, -0.00739918, -0.00739918, -0.00739918,\n",
      "       -0.00739918, -0.00739918, -0.00739918, -0.00739918, -0.00739918,\n",
      "       -0.00739918, -0.00739918, -0.00739918, -0.00739918, -0.00739918,\n",
      "       -0.00739918, -0.00739918, -0.00739918, -0.00739918, -0.00739918,\n",
      "       -0.00739918, -0.00739918, -0.00739918, -0.00739918, -0.00739918,\n",
      "       -0.00739918, -0.00739918, -0.00739918, -0.00739918, -0.00739918,\n",
      "       -0.00739918, -0.00739918, -0.00739918, -0.00739918, -0.00739918,\n",
      "       -4.947619  , -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357,\n",
      "       -0.00882357, -0.00882357, -0.00882357, -0.00882357, -0.00882357],\n",
      "      dtype=float32), 'values': array([-0.08376142, -0.2282035 , -0.26092437, -0.28889498, -0.24340576,\n",
      "       -0.08396176,  0.21812287,  0.5964948 ,  0.5964948 ,  0.5964948 ,\n",
      "        0.5964948 ,  0.5964948 ,  0.5964948 ,  0.5964948 ,  0.5964948 ,\n",
      "        0.5964948 ,  0.5964948 ,  0.5964948 ,  0.5964948 ,  0.5964948 ,\n",
      "        0.5964948 ,  0.5964948 ,  0.5964948 ,  0.5964948 ,  0.5964948 ,\n",
      "        0.5964948 ,  0.5964948 ,  0.5964948 ,  0.5964948 ,  0.5964948 ,\n",
      "        0.5964948 ,  0.5964948 ,  0.5964948 ,  0.5964948 ,  0.5964948 ,\n",
      "        0.5964948 ,  0.5964948 ,  0.5964948 ,  0.5964948 ,  0.5964948 ,\n",
      "        0.5964948 ,  0.5964948 ,  0.5964948 ,  0.5964948 ,  0.5964948 ,\n",
      "        0.5964948 ,  0.5964948 ,  0.5964948 ,  0.5964948 ,  0.5964948 ,\n",
      "        0.5964948 ,  0.5964948 ,  0.5964948 ,  0.5964948 ,  0.5964948 ,\n",
      "        0.5964948 ,  0.5964948 ,  0.5964948 ,  0.5964948 ,  0.5964948 ,\n",
      "        0.5964948 ,  0.5964948 ,  0.5964948 ,  0.5964948 ,  0.5964948 ,\n",
      "        0.5964948 ,  0.5964948 ,  0.5964948 ,  0.5964948 ,  0.5964948 ,\n",
      "        0.5964948 ,  0.5964948 ,  0.5964948 ,  0.5964948 ,  0.5964948 ,\n",
      "        0.5964948 ,  0.5964948 ,  0.5964948 ,  0.5964948 ,  0.5964948 ,\n",
      "        0.5964948 ,  0.5964948 ,  0.5964948 ,  0.5964948 ,  0.5964948 ,\n",
      "        0.5964948 ,  0.5964948 ,  0.5964948 ,  0.5964948 ,  0.5964948 ,\n",
      "        0.5964948 ,  0.5964948 ,  0.5964948 ,  0.5964948 ,  0.789865  ,\n",
      "        1.2385651 ,  1.7162896 ,  2.0944493 ,  2.2999854 ,  2.2999854 ,\n",
      "        2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,\n",
      "        2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,\n",
      "        2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,\n",
      "        2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,\n",
      "        2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,\n",
      "        2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,\n",
      "        2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,\n",
      "        2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,\n",
      "        2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,\n",
      "        2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,\n",
      "        2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,\n",
      "        2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,\n",
      "        2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,\n",
      "        2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,\n",
      "        2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,\n",
      "        2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,\n",
      "        2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,\n",
      "        2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,\n",
      "        2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,\n",
      "        2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,\n",
      "        2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,\n",
      "        2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,\n",
      "        2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,\n",
      "        2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,\n",
      "        2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,\n",
      "        2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,\n",
      "        2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,\n",
      "        2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,  2.2999854 ,\n",
      "        2.2999854 ,  2.2999854 ,  2.351936  ,  2.54555   ,  2.8382413 ,\n",
      "        2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,\n",
      "        2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,\n",
      "        2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,\n",
      "        2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,\n",
      "        2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,\n",
      "        2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,\n",
      "        2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,\n",
      "        2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,\n",
      "        2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,\n",
      "        2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,\n",
      "        2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,\n",
      "        2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,\n",
      "        2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,\n",
      "        2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,\n",
      "        2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,\n",
      "        2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,\n",
      "        2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,\n",
      "        2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,\n",
      "        2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,\n",
      "        2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,\n",
      "        2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,\n",
      "        2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,\n",
      "        2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,\n",
      "        2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,\n",
      "        2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,\n",
      "        2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,\n",
      "        2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,\n",
      "        2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,\n",
      "        2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,\n",
      "        2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,\n",
      "        2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,\n",
      "        2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,\n",
      "        2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,\n",
      "        2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,\n",
      "        2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,\n",
      "        2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,\n",
      "        2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,\n",
      "        2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,\n",
      "        2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,\n",
      "        2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,\n",
      "        2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,\n",
      "        2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,\n",
      "        2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,\n",
      "        2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,  2.8382413 ,\n",
      "        2.8382413 ,  2.8594441 ,  2.8594441 ,  2.8594441 ,  2.8594441 ,\n",
      "        2.8594441 ,  2.8594441 ,  2.8594441 ,  2.8594441 ,  2.8594441 ,\n",
      "        2.8594441 ,  2.8594441 ,  2.8594441 ,  2.8594441 ,  2.8594441 ,\n",
      "        2.8594441 ,  2.8594441 ,  2.8594441 ,  2.8594441 ,  2.8594441 ,\n",
      "        2.8594441 ,  2.8594441 ,  2.8594441 ,  2.8594441 ,  2.8594441 ,\n",
      "        2.8594441 ,  2.8594441 ,  2.8594441 ,  2.8594441 ,  2.8594441 ,\n",
      "        2.8594441 ,  2.8594441 ,  2.8594441 ,  2.8594441 ,  2.8594441 ,\n",
      "        2.8594441 ,  2.8594441 ,  2.8594441 ,  2.8940997 ,  2.8940997 ,\n",
      "        2.8940997 ,  2.8940997 ,  2.8940997 ,  2.8940997 ,  2.8940997 ,\n",
      "        2.8940997 ,  2.8940997 ,  2.8940997 ,  2.8940997 ,  2.8940997 ,\n",
      "        2.8940997 ,  2.8940997 ,  2.8940997 ,  2.8940997 ,  2.8940997 ,\n",
      "        2.8940997 ,  2.8940997 ,  2.8940997 ,  2.8940997 ,  2.8940997 ,\n",
      "        2.8940997 ,  2.8940997 ,  2.8940997 ,  2.8940997 ,  2.8940997 ,\n",
      "        2.8940997 ,  2.8940997 ,  2.8940997 ,  2.8940997 ,  2.8940997 ,\n",
      "        2.8940997 ,  2.8940997 ,  2.8940997 ,  2.8940997 ,  2.8940997 ,\n",
      "        2.8940997 ,  2.8940997 ,  2.8940997 ,  2.8940997 ,  2.8940997 ,\n",
      "        2.8940997 ,  2.8940997 ,  2.8940997 ,  2.8940997 ,  2.8940997 ,\n",
      "        2.8940997 ,  2.8940997 ,  2.8940997 ,  2.8940997 ,  2.8940997 ,\n",
      "        2.8940997 ,  2.8940997 ,  2.8940997 ,  2.8940997 ,  2.8940997 ,\n",
      "        2.8940997 ,  2.8940997 ,  2.8940997 ,  2.8940997 ,  2.8940997 ,\n",
      "        2.8940997 ,  2.8940997 ,  2.8940997 ,  2.8940997 ,  2.8940997 ,\n",
      "        2.8940997 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,\n",
      "        3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ,  3.0110304 ],\n",
      "      dtype=float32), 'advs': array([ 2.13927197e+00,  2.03969383e+00,  1.85635006e+00,  1.66185153e+00,\n",
      "        1.40741706e+00,  1.06259382e+00,  6.04268312e-01,  7.54568875e-02,\n",
      "        7.80803934e-02,  8.08119699e-02,  8.36568251e-02,  8.66195038e-02,\n",
      "        8.97045732e-02,  9.29165855e-02,  9.62620527e-02,  9.97455269e-02,\n",
      "        1.03372872e-01,  1.07150599e-01,  1.11084566e-01,  1.15180627e-01,\n",
      "        1.19445950e-01,  1.23888344e-01,  1.28513664e-01,  1.33331031e-01,\n",
      "        1.38346955e-01,  1.43569887e-01,  1.49009615e-01,  1.54673263e-01,\n",
      "        1.60571933e-01,  1.66714087e-01,  1.73110113e-01,  1.79770440e-01,\n",
      "        1.86706796e-01,  1.93928942e-01,  2.01450542e-01,  2.09282681e-01,\n",
      "        2.17438340e-01,  2.25931883e-01,  2.34775662e-01,  2.43985936e-01,\n",
      "        2.53576398e-01,  2.63563335e-01,  2.73963630e-01,  2.84793586e-01,\n",
      "        2.96070784e-01,  3.07814747e-01,  3.20044339e-01,  3.32779109e-01,\n",
      "        3.46040547e-01,  3.59850764e-01,  3.74231249e-01,  3.89206111e-01,\n",
      "        4.04800713e-01,  4.21039134e-01,  4.37949389e-01,  4.55558807e-01,\n",
      "        4.73896056e-01,  4.92991716e-01,  5.12876391e-01,  5.33583224e-01,\n",
      "        5.55146158e-01,  5.77600956e-01,  6.00983381e-01,  6.25332594e-01,\n",
      "        6.50688827e-01,  6.77092552e-01,  7.04588652e-01,  7.33221412e-01,\n",
      "        7.63037622e-01,  7.94086218e-01,  8.26418579e-01,  8.60088050e-01,\n",
      "        8.95148695e-01,  9.31659162e-01,  9.69679236e-01,  1.00927079e+00,\n",
      "        1.05049968e+00,  1.09343219e+00,  1.13813996e+00,  1.18469656e+00,\n",
      "        1.23317683e+00,  1.28366220e+00,  1.33623397e+00,  1.39097941e+00,\n",
      "        1.44798839e+00,  1.50735402e+00,  1.56917381e+00,  1.63354945e+00,\n",
      "        1.70058632e+00,  1.77039456e+00,  1.84308863e+00,  1.91878831e+00,\n",
      "        1.99761736e+00,  2.07970524e+00,  1.74477136e+00,  1.21768117e+00,\n",
      "        6.51565075e-01,  1.35507956e-01, -2.77715325e-01, -2.77583152e-01,\n",
      "       -2.77445793e-01, -2.77303249e-01, -2.77154148e-01, -2.76999235e-01,\n",
      "       -2.76837140e-01, -2.76669174e-01, -2.76494056e-01, -2.76311785e-01,\n",
      "       -2.76121676e-01, -2.75923789e-01, -2.75718063e-01, -2.75503248e-01,\n",
      "       -2.75279939e-01, -2.75047541e-01, -2.74805367e-01, -2.74553448e-01,\n",
      "       -2.74290442e-01, -2.74017006e-01, -2.73732543e-01, -2.73435682e-01,\n",
      "       -2.73127109e-01, -2.72805512e-01, -2.72470236e-01, -2.72121310e-01,\n",
      "       -2.71758050e-01, -2.71380484e-01, -2.70986617e-01, -2.70576507e-01,\n",
      "       -2.70149440e-01, -2.69704819e-01, -2.69241303e-01, -2.68758923e-01,\n",
      "       -2.68256992e-01, -2.67734259e-01, -2.67189384e-01, -2.66622365e-01,\n",
      "       -2.66031891e-01, -2.65417367e-01, -2.64776796e-01, -2.64110178e-01,\n",
      "       -2.63415545e-01, -2.62692302e-01, -2.61939734e-01, -2.61155307e-01,\n",
      "       -2.60338962e-01, -2.59488761e-01, -2.58603394e-01, -2.57680923e-01,\n",
      "       -2.56721377e-01, -2.55721450e-01, -2.54679829e-01, -2.53595948e-01,\n",
      "       -2.52467096e-01, -2.51291424e-01, -2.50066876e-01, -2.48791590e-01,\n",
      "       -2.47464210e-01, -2.46081501e-01, -2.44642138e-01, -2.43142903e-01,\n",
      "       -2.41581813e-01, -2.39955634e-01, -2.38263041e-01, -2.36500144e-01,\n",
      "       -2.34664336e-01, -2.32752368e-01, -2.30761617e-01, -2.28688851e-01,\n",
      "       -2.26530150e-01, -2.24282265e-01, -2.21941277e-01, -2.19503298e-01,\n",
      "       -2.16965079e-01, -2.14321375e-01, -2.11568967e-01, -2.08702639e-01,\n",
      "       -2.05717817e-01, -2.02609316e-01, -1.99372575e-01, -1.96001723e-01,\n",
      "       -1.92491546e-01, -1.88836202e-01, -1.85029835e-01, -1.81066573e-01,\n",
      "       -1.76938608e-01, -1.72640741e-01, -1.68165147e-01, -1.63504019e-01,\n",
      "       -1.58650845e-01, -1.53596520e-01, -1.48333222e-01, -1.42852500e-01,\n",
      "       -1.37145221e-01, -1.31201625e-01, -1.25013262e-01, -1.18568406e-01,\n",
      "       -1.11856647e-01, -1.04868211e-01, -9.75907296e-02, -9.00118425e-02,\n",
      "       -8.21204782e-02, -7.39023089e-02, -6.53443187e-02, -5.64328432e-02,\n",
      "       -4.71529029e-02, -3.74888778e-02, -2.74257883e-02, -1.69467144e-02,\n",
      "       -6.03472814e-03,  5.32904873e-03,  1.71621945e-02,  2.94848867e-02,\n",
      "        4.23166603e-02,  5.56796417e-02,  6.95940182e-02,  8.40845257e-02,\n",
      "        9.91733000e-02,  1.14886381e-01,  1.31249145e-01,  1.48288310e-01,\n",
      "        1.66031182e-01,  1.84508398e-01,  2.03749210e-01,  2.23785549e-01,\n",
      "        2.44649962e-01,  2.66376942e-01,  2.89002270e-01,  3.12563092e-01,\n",
      "        3.37097824e-01,  3.62646788e-01,  3.89252335e-01,  4.16957378e-01,\n",
      "        4.45808202e-01,  4.75851655e-01,  5.07136524e-01,  5.39715588e-01,\n",
      "        5.73640943e-01,  6.08969152e-01,  6.45757556e-01,  6.84066713e-01,\n",
      "        7.23960340e-01,  7.65502334e-01,  4.87867713e-01,  9.94317457e-02,\n",
      "       -3.73410344e-01, -3.73408407e-01, -3.73406440e-01, -3.73404503e-01,\n",
      "       -3.73402536e-01, -3.73399943e-01, -3.73397976e-01, -3.73395383e-01,\n",
      "       -3.73392761e-01, -3.73390168e-01, -3.73387575e-01, -3.73384297e-01,\n",
      "       -3.73381704e-01, -3.73378456e-01, -3.73375207e-01, -3.73371929e-01,\n",
      "       -3.73368680e-01, -3.73364776e-01, -3.73360872e-01, -3.73356968e-01,\n",
      "       -3.73353064e-01, -3.73349160e-01, -3.73344600e-01, -3.73340040e-01,\n",
      "       -3.73334825e-01, -3.73330265e-01, -3.73325080e-01, -3.73319209e-01,\n",
      "       -3.73313993e-01, -3.73308152e-01, -3.73301625e-01, -3.73295128e-01,\n",
      "       -3.73288602e-01, -3.73282105e-01, -3.73274952e-01, -3.73267144e-01,\n",
      "       -3.73259306e-01, -3.73251498e-01, -3.73243034e-01, -3.73233944e-01,\n",
      "       -3.73224825e-01, -3.73215050e-01, -3.73205274e-01, -3.73194873e-01,\n",
      "       -3.73183817e-01, -3.73172730e-01, -3.73161018e-01, -3.73148650e-01,\n",
      "       -3.73136282e-01, -3.73123258e-01, -3.73108953e-01, -3.73094618e-01,\n",
      "       -3.73080283e-01, -3.73064667e-01, -3.73048395e-01, -3.73031467e-01,\n",
      "       -3.73013884e-01, -3.72995675e-01, -3.72976780e-01, -3.72956604e-01,\n",
      "       -3.72935772e-01, -3.72914284e-01, -3.72892171e-01, -3.72868717e-01,\n",
      "       -3.72844636e-01, -3.72819245e-01, -3.72793198e-01, -3.72765869e-01,\n",
      "       -3.72737229e-01, -3.72707278e-01, -3.72676671e-01, -3.72644126e-01,\n",
      "       -3.72610271e-01, -3.72575760e-01, -3.72539312e-01, -3.72501552e-01,\n",
      "       -3.72461855e-01, -3.72420847e-01, -3.72377872e-01, -3.72333616e-01,\n",
      "       -3.72286737e-01, -3.72238547e-01, -3.72188419e-01, -3.72135699e-01,\n",
      "       -3.72081012e-01, -3.72024387e-01, -3.71965140e-01, -3.71903956e-01,\n",
      "       -3.71839494e-01, -3.71772438e-01, -3.71702790e-01, -3.71630520e-01,\n",
      "       -3.71555001e-01, -3.71476889e-01, -3.71394873e-01, -3.71309578e-01,\n",
      "       -3.71221066e-01, -3.71128619e-01, -3.71032268e-01, -3.70932013e-01,\n",
      "       -3.70827854e-01, -3.70719135e-01, -3.70605856e-01, -3.70488018e-01,\n",
      "       -3.70364994e-01, -3.70237410e-01, -3.70104581e-01, -3.69965941e-01,\n",
      "       -3.69821399e-01, -3.69671047e-01, -3.69514793e-01, -3.69352043e-01,\n",
      "       -3.69182140e-01, -3.69005710e-01, -3.68821472e-01, -3.68630081e-01,\n",
      "       -3.68430227e-01, -3.68222564e-01, -3.68006438e-01, -3.67781192e-01,\n",
      "       -3.67546171e-01, -3.67302060e-01, -3.67047518e-01, -3.66782576e-01,\n",
      "       -3.66507202e-01, -3.66219461e-01, -3.65920633e-01, -3.65608811e-01,\n",
      "       -3.65284622e-01, -3.64946753e-01, -3.64595234e-01, -3.64228696e-01,\n",
      "       -3.63847226e-01, -3.63450110e-01, -3.63036096e-01, -3.62605780e-01,\n",
      "       -3.62157255e-01, -3.61689836e-01, -3.61203521e-01, -3.60697061e-01,\n",
      "       -3.60169113e-01, -3.59620303e-01, -3.59048098e-01, -3.58452439e-01,\n",
      "       -3.57832015e-01, -3.57186228e-01, -3.56513768e-01, -3.55813295e-01,\n",
      "       -3.55083525e-01, -3.54324460e-01, -3.53533506e-01, -3.52710009e-01,\n",
      "       -3.51851970e-01, -3.50958824e-01, -3.50029200e-01, -3.49060506e-01,\n",
      "       -3.48052114e-01, -3.47001404e-01, -3.45907748e-01, -3.44769150e-01,\n",
      "       -3.43583047e-01, -3.42348099e-01, -3.41062397e-01, -3.39722633e-01,\n",
      "       -3.38328212e-01, -3.36875826e-01, -3.35363567e-01, -3.33788812e-01,\n",
      "       -3.32148969e-01, -3.30441386e-01, -3.28663528e-01, -3.26811433e-01,\n",
      "       -3.24883193e-01, -3.22874874e-01, -3.20783883e-01, -3.18606287e-01,\n",
      "       -3.16338867e-01, -3.13977718e-01, -3.11518908e-01, -3.08957905e-01,\n",
      "       -3.06291401e-01, -3.03514928e-01, -3.00623208e-01, -2.97612339e-01,\n",
      "       -2.94477135e-01, -2.91211754e-01, -2.87811607e-01, -2.84271508e-01,\n",
      "       -2.80584246e-01, -2.76744694e-01, -2.72746921e-01, -2.68583149e-01,\n",
      "       -2.64247537e-01, -2.59732872e-01, -2.55031377e-01, -2.50135243e-01,\n",
      "       -2.45037302e-01, -2.39728436e-01, -2.34200180e-01, -2.28442773e-01,\n",
      "       -2.22447753e-01, -2.16204703e-01, -2.09703863e-01, -2.02934161e-01,\n",
      "       -1.95884541e-01, -1.88543916e-01, -1.80899277e-01, -1.72938898e-01,\n",
      "       -1.64649129e-01, -1.56016916e-01, -1.47027329e-01, -1.37666658e-01,\n",
      "       -1.27918661e-01, -1.17767684e-01, -1.07197471e-01, -9.61897895e-02,\n",
      "       -8.47277120e-02, -7.27910548e-02, -6.03609495e-02, -4.74172086e-02,\n",
      "       -3.39389965e-02, -1.99028850e-02, -3.04542840e-01, -3.01543057e-01,\n",
      "       -2.98419565e-01, -2.95166552e-01, -2.91779429e-01, -2.88251668e-01,\n",
      "       -2.84578770e-01, -2.80753523e-01, -2.76770711e-01, -2.72622585e-01,\n",
      "       -2.68303216e-01, -2.63805509e-01, -2.59121597e-01, -2.54244328e-01,\n",
      "       -2.49165267e-01, -2.43875921e-01, -2.38368496e-01, -2.32633233e-01,\n",
      "       -2.26660341e-01, -2.20441386e-01, -2.13964641e-01, -2.07220316e-01,\n",
      "       -2.00197384e-01, -1.92884102e-01, -1.85268104e-01, -1.77337021e-01,\n",
      "       -1.69079155e-01, -1.60478845e-01, -1.51523754e-01, -1.42197594e-01,\n",
      "       -1.32486686e-01, -1.22374125e-01, -1.11842975e-01, -1.00876957e-01,\n",
      "       -8.94571915e-02, -7.75661096e-02, -6.51828721e-02, -3.61010849e-01,\n",
      "       -3.60099435e-01, -3.59150290e-01, -3.58162075e-01, -3.57132852e-01,\n",
      "       -3.56060684e-01, -3.54944199e-01, -3.53782177e-01, -3.52571994e-01,\n",
      "       -3.51310998e-01, -3.49998593e-01, -3.48632157e-01, -3.47208440e-01,\n",
      "       -3.45726103e-01, -3.44182611e-01, -3.42575938e-01, -3.40902239e-01,\n",
      "       -3.39158863e-01, -3.37343901e-01, -3.35454047e-01, -3.33486110e-01,\n",
      "       -3.31436753e-01, -3.29302162e-01, -3.27079654e-01, -3.24765354e-01,\n",
      "       -3.22355390e-01, -3.19845796e-01, -3.17232698e-01, -3.14511538e-01,\n",
      "       -3.11677098e-01, -3.08726132e-01, -3.05653453e-01, -3.02453160e-01,\n",
      "       -2.99120694e-01, -2.95650244e-01, -2.92037219e-01, -2.88273811e-01,\n",
      "       -2.84355462e-01, -2.80274391e-01, -2.76025325e-01, -2.71600515e-01,\n",
      "       -2.66992778e-01, -2.62194276e-01, -2.57197231e-01, -2.51993835e-01,\n",
      "       -2.46575594e-01, -2.40933418e-01, -2.35057533e-01, -2.28938833e-01,\n",
      "       -2.22566888e-01, -2.15931937e-01, -2.09022924e-01, -2.01828122e-01,\n",
      "       -1.94335818e-01, -1.86533645e-01, -1.78408563e-01, -1.69948220e-01,\n",
      "       -1.61137655e-01, -1.51963174e-01, -1.42409161e-01, -1.32460654e-01,\n",
      "       -1.22100711e-01, -1.11311764e-01, -1.00077532e-01, -8.83784965e-02,\n",
      "       -7.61964172e-02, -6.35098144e-02, -5.02998158e-02, -4.03166056e-01,\n",
      "       -4.03166056e-01, -4.03166056e-01, -4.03166056e-01, -4.03166056e-01,\n",
      "       -4.03166056e-01, -4.03166056e-01, -4.03166056e-01, -4.03166056e-01,\n",
      "       -4.03166056e-01, -4.03165400e-01, -4.03165400e-01, -4.03165400e-01,\n",
      "       -4.03165400e-01, -4.03165400e-01, -4.03165400e-01, -4.03165400e-01,\n",
      "       -4.03165400e-01, -4.03165400e-01, -4.03165400e-01, -4.03165400e-01,\n",
      "       -4.03165400e-01, -4.03165400e-01, -4.03165400e-01, -4.03165400e-01,\n",
      "       -4.03165400e-01, -4.03165400e-01, -4.03165400e-01, -4.03165400e-01,\n",
      "       -4.03165400e-01, -4.03165400e-01, -4.03165400e-01, -4.03165400e-01,\n",
      "       -4.03165400e-01, -4.03165400e-01, -4.03164744e-01, -4.03164744e-01,\n",
      "       -4.03164744e-01, -4.03164744e-01, -4.03164744e-01, -4.03164744e-01,\n",
      "       -4.03164744e-01, -4.03164744e-01, -4.03164744e-01, -4.03164744e-01,\n",
      "       -4.03164744e-01, -4.03164744e-01, -4.03164089e-01, -4.03164089e-01,\n",
      "       -4.03164089e-01, -4.03164089e-01, -4.03164089e-01, -4.03164089e-01,\n",
      "       -4.03164089e-01, -4.03164089e-01, -4.03163433e-01, -4.03163433e-01,\n",
      "       -4.03163433e-01, -4.03163433e-01, -4.03163433e-01, -4.03163433e-01,\n",
      "       -4.03162807e-01, -4.03162807e-01, -4.03162807e-01, -4.03162807e-01,\n",
      "       -4.03162807e-01, -4.03162152e-01, -4.03162152e-01, -4.03162152e-01,\n",
      "       -4.03162152e-01, -4.03161496e-01, -4.03161496e-01, -4.03161496e-01,\n",
      "       -4.03160840e-01, -4.03160840e-01, -4.03160840e-01, -4.03160185e-01,\n",
      "       -4.03160185e-01, -4.03160185e-01, -4.03159529e-01, -4.03159529e-01,\n",
      "       -4.03159529e-01, -4.03158903e-01, -4.03158903e-01, -4.03158247e-01,\n",
      "       -4.03158247e-01, -4.03157592e-01, -4.03157592e-01, -4.03156936e-01,\n",
      "       -4.03156281e-01, -4.03156281e-01, -4.03155625e-01, -4.03155625e-01,\n",
      "       -4.03154999e-01, -4.03154343e-01, -4.03153688e-01, -4.03153688e-01,\n",
      "       -4.03153032e-01, -4.03152376e-01, -4.03151721e-01, -4.03151065e-01,\n",
      "       -4.03150439e-01, -4.03149784e-01, -4.03149128e-01, -4.03148472e-01,\n",
      "       -4.03147817e-01, -4.03147161e-01, -4.03146535e-01, -4.03145880e-01,\n",
      "       -4.03144568e-01, -4.03143913e-01, -4.03142601e-01, -4.03141975e-01,\n",
      "       -4.03140664e-01, -4.03140008e-01, -4.03138697e-01, -4.03137416e-01,\n",
      "       -4.03136760e-01, -4.03135449e-01, -4.03134167e-01, -4.03132856e-01,\n",
      "       -4.03131545e-01, -4.03129607e-01, -4.03128296e-01, -4.03126985e-01,\n",
      "       -4.03125048e-01, -4.03123736e-01, -4.03121769e-01, -4.03119832e-01,\n",
      "       -4.03117865e-01, -4.03115928e-01, -4.03113961e-01, -4.03112024e-01,\n",
      "       -4.03109401e-01, -4.03107464e-01, -4.03104872e-01, -4.03102249e-01,\n",
      "       -4.03099656e-01, -4.03097034e-01, -4.03093785e-01, -4.03091192e-01,\n",
      "       -4.03087944e-01, -4.03084666e-01, -4.03081417e-01, -4.03078169e-01,\n",
      "       -4.03074265e-01, -4.03070360e-01, -4.03066456e-01, -4.03062552e-01,\n",
      "       -4.03057992e-01, -4.03053433e-01, -4.03048873e-01, -4.03044313e-01,\n",
      "       -4.03039098e-01, -4.03033912e-01, -4.03028041e-01, -4.03022826e-01,\n",
      "       -4.03016984e-01, -4.03010458e-01, -4.03003961e-01, -4.02997434e-01,\n",
      "       -4.02990282e-01, -4.02983129e-01, -4.02975321e-01, -4.02967513e-01,\n",
      "       -4.02959675e-01, -4.02951211e-01, -4.02942121e-01, -4.02933002e-01,\n",
      "       -4.02923226e-01, -4.02912825e-01, -4.02902395e-01, -4.02891994e-01,\n",
      "       -4.02880251e-01, -4.02868539e-01, -4.02856171e-01, -4.02843148e-01,\n",
      "       -4.02830124e-01, -4.02816474e-01, -4.02801484e-01, -4.02786523e-01,\n",
      "       -4.02770907e-01, -4.02754635e-01, -4.02737707e-01, -4.02720124e-01,\n",
      "       -4.02701229e-01, -4.02682364e-01, -4.02662188e-01, -4.02641356e-01,\n",
      "       -4.02619869e-01, -4.02597070e-01, -4.02573645e-01, -4.02548909e-01,\n",
      "       -4.02523518e-01, -4.02497470e-01, -4.02469486e-01, -4.02440846e-01,\n",
      "       -4.02410895e-01, -4.02379662e-01, -4.02347088e-01, -4.02313232e-01,\n",
      "       -4.02278095e-01, -4.02240992e-01, -4.02202576e-01, -4.02162880e-01,\n",
      "       -4.02121842e-01, -4.02078241e-01, -4.02033329e-01, -4.01986450e-01,\n",
      "       -4.01937634e-01, -4.01886851e-01, -4.01834100e-01, -4.01779443e-01,\n",
      "       -4.01722133e-01, -4.01662260e-01, -4.01599765e-01, -4.01535302e-01,\n",
      "       -4.01467592e-01, -4.01397288e-01, -4.01324391e-01, -4.01248217e-01,\n",
      "       -4.01168793e-01, -4.01086777e-01, -4.01000202e-01, -4.00911003e-01,\n",
      "       -4.00817901e-01, -4.00720924e-01, -4.00619358e-01, -4.00514543e-01,\n",
      "       -4.00404543e-01, -4.00290608e-01, -4.00171459e-01, -4.00047779e-01,\n",
      "       -3.99918884e-01, -3.99784774e-01, -3.99644822e-01, -3.99498999e-01,\n",
      "       -3.99347961e-01, -3.99189770e-01, -3.99025708e-01, -3.98854494e-01,\n",
      "       -3.98676127e-01, -3.98490608e-01, -3.98297250e-01, -3.98096085e-01,\n",
      "       -3.97886485e-01, -3.97667736e-01, -3.97440553e-01, -3.97203594e-01,\n",
      "       -3.96957517e-01, -3.96701008e-01, -3.96433443e-01, -3.96154821e-01,\n",
      "       -3.95865142e-01, -3.95563722e-01, -3.95249307e-01, -3.94921839e-01,\n",
      "       -3.94580722e-01, -3.94225925e-01, -3.93856168e-01, -3.93471420e-01,\n",
      "       -3.93071055e-01, -3.92653763e-01, -3.92218918e-01, -3.91766459e-01,\n",
      "       -3.91295135e-01, -3.90804291e-01, -3.90293270e-01, -3.89760762e-01,\n",
      "       -3.89206767e-01, -3.88629973e-01, -3.88029099e-01, -3.87402833e-01,\n",
      "       -3.86751205e-01, -3.86072874e-01, -3.85365874e-01, -3.84630263e-01,\n",
      "       -3.83864045e-01, -3.83065909e-01, -3.82235259e-01, -3.81369412e-01,\n",
      "       -3.80468458e-01, -3.79530370e-01, -3.78553212e-01, -3.77535701e-01,\n",
      "       -3.76475900e-01, -3.75372440e-01, -3.74223441e-01, -3.73026907e-01,\n",
      "       -3.71780902e-01, -3.70483488e-01, -3.69132668e-01, -3.67725194e-01,\n",
      "       -3.66260469e-01, -3.64734530e-01, -3.63145441e-01, -3.61491263e-01,\n",
      "       -3.59768093e-01, -3.57973933e-01, -3.56105596e-01, -3.54160428e-01,\n",
      "       -3.52134526e-01, -3.50024641e-01, -3.47827524e-01, -3.45539927e-01,\n",
      "       -3.43157291e-01, -3.40676993e-01, -3.38093191e-01, -3.35403293e-01,\n",
      "       -3.32602054e-01, -3.29684287e-01, -3.26646745e-01, -3.23483557e-01,\n",
      "       -3.20188880e-01, -3.16758782e-01, -3.13186765e-01, -3.09466958e-01,\n",
      "       -3.05592895e-01, -3.01559329e-01, -2.97358453e-01, -2.92984426e-01,\n",
      "       -2.88429409e-01, -2.83686250e-01, -2.78746486e-01, -2.73602992e-01,\n",
      "       -2.68246591e-01, -2.62668848e-01, -2.56860673e-01, -2.50812292e-01,\n",
      "       -2.44513899e-01, -2.37955123e-01, -2.31124878e-01, -2.24012747e-01,\n",
      "       -2.16605723e-01, -2.08893374e-01, -2.00862050e-01, -1.92498058e-01,\n",
      "       -1.83789045e-01, -1.74720019e-01, -1.65275380e-01, -1.55440792e-01,\n",
      "       -1.45199329e-01, -1.34534717e-01, -1.23429395e-01, -1.11864455e-01,\n",
      "       -9.98216942e-02, -8.72815698e-02, -7.42219463e-02, -6.06233031e-02,\n",
      "       -4.64621969e-02, -3.17158476e-02, -1.63595155e-02, -3.68463428e-04,\n",
      "        1.62840001e-02,  3.36245671e-02,  5.16818799e-02,  7.04858825e-02,\n",
      "        9.00671780e-02,  1.10458307e-01,  1.31691828e-01,  1.53803542e-01,\n",
      "        1.76829904e-01,  2.00807363e-01,  2.25776300e-01,  2.51777709e-01,\n",
      "        2.78853923e-01,  3.07049185e-01,  3.36410373e-01,  3.66985679e-01,\n",
      "        3.98824573e-01,  4.31979775e-01,  4.66505945e-01,  5.02459109e-01,\n",
      "        5.39899170e-01,  5.78886628e-01,  6.19485974e-01,  6.61763608e-01,\n",
      "        7.05789089e-01,  7.51634181e-01,  7.99375594e-01,  8.49090159e-01,\n",
      "        9.00859892e-01,  9.54770148e-01,  1.01090872e+00,  1.06936812e+00,\n",
      "        1.13024461e+00,  1.19363797e+00,  1.25965154e+00,  1.32839465e+00,\n",
      "        1.39997947e+00,  1.47452366e+00,  1.55214965e+00,  1.63298500e+00,\n",
      "        1.71716201e+00,  1.80481923e+00,  1.89609981e+00,  1.99115431e+00,\n",
      "        2.09013867e+00,  2.19321489e+00,  2.30055213e+00,  2.41232753e+00,\n",
      "        2.52872348e+00,  2.64993167e+00,  2.77615070e+00,  2.90758729e+00,\n",
      "        3.04445767e+00,  3.18698740e+00,  3.33540845e+00,  3.48996592e+00,\n",
      "        3.65091276e+00,  3.81851363e+00,  3.99304318e+00,  4.17478752e+00,\n",
      "        4.36404610e+00,  4.56112862e+00,  4.76635885e+00,  4.98007345e+00,\n",
      "        5.20262289e+00,  5.43437338e+00,  5.67570448e+00,  5.92701292e+00,\n",
      "        6.18871069e+00,  6.46122646e+00,  6.74500895e+00,  7.04052401e+00,\n",
      "        7.34825516e+00,  7.66870832e+00,  8.00240993e+00,  8.34990692e+00],\n",
      "      dtype=float32), 'returns': array([ -9.712127  , -10.002446  , -10.303756  , -10.616656  ,\n",
      "       -10.9439    , -11.289604  , -11.658943  , -12.055252  ,\n",
      "       -12.051409  , -12.047407  , -12.04324   , -12.038899  ,\n",
      "       -12.03438   , -12.029675  , -12.024774  , -12.0196705 ,\n",
      "       -12.014357  , -12.008822  , -12.003059  , -11.997059  ,\n",
      "       -11.99081   , -11.9843025 , -11.977527  , -11.970469  ,\n",
      "       -11.963121  , -11.95547   , -11.947501  , -11.939204  ,\n",
      "       -11.930563  , -11.921565  , -11.912195  , -11.902438  ,\n",
      "       -11.892277  , -11.881697  , -11.870678  , -11.859204  ,\n",
      "       -11.847257  , -11.834814  , -11.821858  , -11.808366  ,\n",
      "       -11.794316  , -11.779686  , -11.76445   , -11.748585  ,\n",
      "       -11.732064  , -11.71486   , -11.696944  , -11.678288  ,\n",
      "       -11.658861  , -11.63863   , -11.617563  , -11.595626  ,\n",
      "       -11.572781  , -11.548992  , -11.5242195 , -11.498423  ,\n",
      "       -11.47156   , -11.443585  , -11.414455  , -11.384121  ,\n",
      "       -11.352532  , -11.319637  , -11.285383  , -11.249713  ,\n",
      "       -11.212567  , -11.173887  , -11.133607  , -11.091661  ,\n",
      "       -11.047982  , -11.002498  , -10.9551325 , -10.905808  ,\n",
      "       -10.854446  , -10.800961  , -10.745263  , -10.6872635 ,\n",
      "       -10.626865  , -10.5639715 , -10.498477  , -10.430274  ,\n",
      "       -10.359253  , -10.285295  , -10.20828   , -10.12808   ,\n",
      "       -10.044565  ,  -9.957598  ,  -9.867035  ,  -9.772728  ,\n",
      "        -9.674522  ,  -9.572257  ,  -9.465764  ,  -9.354868  ,\n",
      "        -9.2393875 ,  -9.119133  ,  -9.416424  ,  -9.739882  ,\n",
      "       -10.091489  , -10.469326  , -10.869141  , -10.868946  ,\n",
      "       -10.868746  , -10.868536  , -10.868319  , -10.868092  ,\n",
      "       -10.867853  , -10.867607  , -10.867352  , -10.8670845 ,\n",
      "       -10.866806  , -10.866516  , -10.866215  , -10.8659    ,\n",
      "       -10.865572  , -10.865232  , -10.864878  , -10.864508  ,\n",
      "       -10.864122  , -10.863722  , -10.863306  , -10.862871  ,\n",
      "       -10.862419  , -10.861948  , -10.861456  , -10.860945  ,\n",
      "       -10.860413  , -10.859859  , -10.859283  , -10.858683  ,\n",
      "       -10.858057  , -10.857405  , -10.856726  , -10.85602   ,\n",
      "       -10.855284  , -10.854519  , -10.85372   , -10.85289   ,\n",
      "       -10.852024  , -10.851124  , -10.850185  , -10.849209  ,\n",
      "       -10.848192  , -10.847132  , -10.846029  , -10.844881  ,\n",
      "       -10.843685  , -10.84244   , -10.841143  , -10.83979   ,\n",
      "       -10.838385  , -10.83692   , -10.835394  , -10.833807  ,\n",
      "       -10.832153  , -10.830431  , -10.828636  , -10.826769  ,\n",
      "       -10.824823  , -10.822798  , -10.82069   , -10.818493  ,\n",
      "       -10.816206  , -10.813824  , -10.811344  , -10.808762  ,\n",
      "       -10.806072  , -10.803272  , -10.800356  , -10.797319  ,\n",
      "       -10.794157  , -10.790863  , -10.787434  , -10.783863  ,\n",
      "       -10.780144  , -10.776272  , -10.77224   , -10.76804   ,\n",
      "       -10.763668  , -10.759113  , -10.754372  , -10.7494335 ,\n",
      "       -10.744291  , -10.738937  , -10.73336   , -10.727554  ,\n",
      "       -10.721508  , -10.715212  , -10.708654  , -10.701826  ,\n",
      "       -10.694717  , -10.687313  , -10.679602  , -10.671574  ,\n",
      "       -10.663212  , -10.654505  , -10.645439  , -10.635998  ,\n",
      "       -10.626165  , -10.615929  , -10.605267  , -10.594164  ,\n",
      "       -10.582603  , -10.570564  , -10.558027  , -10.544973  ,\n",
      "       -10.531378  , -10.517221  , -10.50248   , -10.487127  ,\n",
      "       -10.471142  , -10.454494  , -10.4371605 , -10.419107  ,\n",
      "       -10.4003105 , -10.3807335 , -10.36035   , -10.339123  ,\n",
      "       -10.3170185 , -10.293999  , -10.270029  , -10.245068  ,\n",
      "       -10.219074  , -10.192007  , -10.16382   , -10.134468  ,\n",
      "       -10.103903  , -10.072073  , -10.038929  , -10.004414  ,\n",
      "        -9.968472  ,  -9.931044  ,  -9.892067  ,  -9.8514805 ,\n",
      "        -9.809216  ,  -9.765203  ,  -9.719374  ,  -9.671646  ,\n",
      "        -9.621948  ,  -9.570194  ,  -9.5163    ,  -9.46018   ,\n",
      "        -9.401737  ,  -9.340881  ,  -9.69565   , -10.0710745 ,\n",
      "       -10.471072  , -10.471069  , -10.471066  , -10.471064  ,\n",
      "       -10.471061  , -10.471057  , -10.471054  , -10.47105   ,\n",
      "       -10.471046  , -10.471043  , -10.471039  , -10.471034  ,\n",
      "       -10.47103   , -10.471025  , -10.471021  , -10.471016  ,\n",
      "       -10.471011  , -10.471005  , -10.471     , -10.470994  ,\n",
      "       -10.470988  , -10.470983  , -10.470976  , -10.470969  ,\n",
      "       -10.470962  , -10.470955  , -10.470947  , -10.470939  ,\n",
      "       -10.470931  , -10.470922  , -10.470913  , -10.470903  ,\n",
      "       -10.470894  , -10.470884  , -10.470874  , -10.470862  ,\n",
      "       -10.470851  , -10.4708395 , -10.470827  , -10.470814  ,\n",
      "       -10.4708    , -10.470786  , -10.470772  , -10.470757  ,\n",
      "       -10.47074   , -10.470724  , -10.470707  , -10.470689  ,\n",
      "       -10.470671  , -10.470652  , -10.470631  , -10.47061   ,\n",
      "       -10.470589  , -10.470566  , -10.470542  , -10.470517  ,\n",
      "       -10.470491  , -10.470465  , -10.470437  , -10.4704075 ,\n",
      "       -10.470377  , -10.4703455 , -10.470313  , -10.470279  ,\n",
      "       -10.470243  , -10.470206  , -10.470168  , -10.470128  ,\n",
      "       -10.470086  , -10.470042  , -10.469997  , -10.46995   ,\n",
      "       -10.4699    , -10.46985   , -10.469796  , -10.469741  ,\n",
      "       -10.469683  , -10.469623  , -10.46956   , -10.469495  ,\n",
      "       -10.469426  , -10.469356  , -10.469282  , -10.469205  ,\n",
      "       -10.469125  , -10.469042  , -10.468955  , -10.468865  ,\n",
      "       -10.468771  , -10.468673  , -10.468571  , -10.468465  ,\n",
      "       -10.468354  , -10.46824   , -10.46812   , -10.467995  ,\n",
      "       -10.467865  , -10.46773   , -10.467588  , -10.467442  ,\n",
      "       -10.467289  , -10.46713   , -10.466964  , -10.466791  ,\n",
      "       -10.466611  , -10.466424  , -10.466229  , -10.466026  ,\n",
      "       -10.465815  , -10.465594  , -10.465365  , -10.465127  ,\n",
      "       -10.464878  , -10.46462   , -10.46435   , -10.464069  ,\n",
      "       -10.463777  , -10.463472  , -10.463156  , -10.462826  ,\n",
      "       -10.4624815 , -10.462124  , -10.461751  , -10.461363  ,\n",
      "       -10.460959  , -10.460538  , -10.4601    , -10.459643  ,\n",
      "       -10.459168  , -10.4586735 , -10.4581585 , -10.457622  ,\n",
      "       -10.457063  , -10.456481  , -10.455874  , -10.455244  ,\n",
      "       -10.454587  , -10.453902  , -10.45319   , -10.452448  ,\n",
      "       -10.451674  , -10.4508705 , -10.450032  , -10.44916   ,\n",
      "       -10.448251  , -10.447305  , -10.44632   , -10.445293  ,\n",
      "       -10.444224  , -10.443112  , -10.441954  , -10.440747  ,\n",
      "       -10.43949   , -10.438182  , -10.43682   , -10.435401  ,\n",
      "       -10.433924  , -10.4323845 , -10.430782  , -10.429114  ,\n",
      "       -10.427377  , -10.425568  , -10.423684  , -10.421721  ,\n",
      "       -10.419679  , -10.417551  , -10.415336  , -10.413029  ,\n",
      "       -10.410626  , -10.408125  , -10.40552   , -10.402807  ,\n",
      "       -10.399982  , -10.39704   , -10.393977  , -10.390787  ,\n",
      "       -10.3874655 , -10.3840065 , -10.380404  , -10.376653  ,\n",
      "       -10.372746  , -10.368679  , -10.364443  , -10.360032  ,\n",
      "       -10.355439  , -10.350656  , -10.3456745 , -10.340488  ,\n",
      "       -10.335087  , -10.329462  , -10.323606  , -10.317506  ,\n",
      "       -10.311154  , -10.304541  , -10.297653  , -10.290481  ,\n",
      "       -10.283012  , -10.275235  , -10.267137  , -10.258702  ,\n",
      "       -10.24992   , -10.240774  , -10.231251  , -10.2213335 ,\n",
      "       -10.211006  , -10.200253  , -10.189054  , -10.177392  ,\n",
      "       -10.165248  , -10.152602  , -10.139433  , -10.12572   ,\n",
      "       -10.11144   , -10.096569  , -10.081084  , -10.064959  ,\n",
      "       -10.048167  , -10.030681  , -10.012471  ,  -9.993509  ,\n",
      "        -9.973764  ,  -9.953202  , -10.348982  , -10.344587  ,\n",
      "       -10.340012  , -10.335247  , -10.330284  , -10.325117  ,\n",
      "       -10.3197365 , -10.314133  , -10.308298  , -10.302221  ,\n",
      "       -10.295893  , -10.289305  , -10.282442  , -10.275297  ,\n",
      "       -10.267857  , -10.260109  , -10.252041  , -10.243639  ,\n",
      "       -10.234888  , -10.225779  , -10.2162895 , -10.206409  ,\n",
      "       -10.196121  , -10.185408  , -10.174252  , -10.162632  ,\n",
      "       -10.150536  , -10.137936  , -10.124817  , -10.111155  ,\n",
      "       -10.09693   , -10.082115  , -10.066687  , -10.050623  ,\n",
      "       -10.033894  , -10.016474  ,  -9.998333  , -10.397049  ,\n",
      "       -10.395714  , -10.394323  , -10.392876  , -10.391369  ,\n",
      "       -10.389797  , -10.388163  , -10.386459  , -10.384687  ,\n",
      "       -10.382839  , -10.380917  , -10.378916  , -10.376829  ,\n",
      "       -10.374659  , -10.372396  , -10.370043  , -10.367592  ,\n",
      "       -10.365038  , -10.362379  , -10.35961   , -10.356728  ,\n",
      "       -10.353725  , -10.350597  , -10.347342  , -10.343952  ,\n",
      "       -10.340422  , -10.336744  , -10.332916  , -10.32893   ,\n",
      "       -10.324778  , -10.320456  , -10.315954  , -10.311266  ,\n",
      "       -10.306383  , -10.3013    , -10.296007  , -10.290493  ,\n",
      "       -10.284754  , -10.278774  , -10.272551  , -10.2660675 ,\n",
      "       -10.259317  , -10.252289  , -10.244968  , -10.237345  ,\n",
      "       -10.229408  , -10.221142  , -10.212534  , -10.203571  ,\n",
      "       -10.194237  , -10.184517  , -10.174395  , -10.163855  ,\n",
      "       -10.15288   , -10.141449  , -10.129547  , -10.117153  ,\n",
      "       -10.104246  , -10.090805  , -10.076809  , -10.062235  ,\n",
      "       -10.047058  , -10.031254  , -10.014795  ,  -9.997658  ,\n",
      "        -9.979811  ,  -9.9612255 ,  -9.941874  , -10.341874  ,\n",
      "       -10.341874  , -10.341874  , -10.341874  , -10.341874  ,\n",
      "       -10.341874  , -10.341874  , -10.341874  , -10.341874  ,\n",
      "       -10.341874  , -10.341873  , -10.341873  , -10.341873  ,\n",
      "       -10.341873  , -10.341873  , -10.341873  , -10.341873  ,\n",
      "       -10.341873  , -10.341873  , -10.341873  , -10.341873  ,\n",
      "       -10.341873  , -10.341873  , -10.341873  , -10.341873  ,\n",
      "       -10.341873  , -10.341873  , -10.341873  , -10.341873  ,\n",
      "       -10.341873  , -10.341873  , -10.341873  , -10.341873  ,\n",
      "       -10.341873  , -10.341873  , -10.341872  , -10.341872  ,\n",
      "       -10.341872  , -10.341872  , -10.341872  , -10.341872  ,\n",
      "       -10.341872  , -10.341872  , -10.341872  , -10.341872  ,\n",
      "       -10.341872  , -10.341872  , -10.341871  , -10.341871  ,\n",
      "       -10.341871  , -10.341871  , -10.341871  , -10.341871  ,\n",
      "       -10.341871  , -10.341871  , -10.34187   , -10.34187   ,\n",
      "       -10.34187   , -10.34187   , -10.34187   , -10.34187   ,\n",
      "       -10.341869  , -10.341869  , -10.341869  , -10.341869  ,\n",
      "       -10.341869  , -10.341868  , -10.341868  , -10.341868  ,\n",
      "       -10.341868  , -10.341867  , -10.341867  , -10.341867  ,\n",
      "       -10.3418665 , -10.3418665 , -10.3418665 , -10.341866  ,\n",
      "       -10.341866  , -10.341866  , -10.341865  , -10.341865  ,\n",
      "       -10.341865  , -10.341864  , -10.341864  , -10.341863  ,\n",
      "       -10.341863  , -10.341862  , -10.341862  , -10.341861  ,\n",
      "       -10.34186   , -10.34186   , -10.341859  , -10.341859  ,\n",
      "       -10.341858  , -10.341857  , -10.341856  , -10.341856  ,\n",
      "       -10.341855  , -10.341854  , -10.341853  , -10.341852  ,\n",
      "       -10.341851  , -10.34185   , -10.341849  , -10.341848  ,\n",
      "       -10.341847  , -10.341846  , -10.3418455 , -10.341845  ,\n",
      "       -10.341843  , -10.341842  , -10.34184   , -10.341839  ,\n",
      "       -10.341837  , -10.341836  , -10.341834  , -10.341832  ,\n",
      "       -10.341831  , -10.341829  , -10.341827  , -10.3418255 ,\n",
      "       -10.341824  , -10.341821  , -10.341819  , -10.341817  ,\n",
      "       -10.341814  , -10.341812  , -10.341809  , -10.341806  ,\n",
      "       -10.341804  , -10.341801  , -10.341798  , -10.341795  ,\n",
      "       -10.341791  , -10.341788  , -10.3417845 , -10.341781  ,\n",
      "       -10.341777  , -10.341773  , -10.341768  , -10.341764  ,\n",
      "       -10.34176   , -10.341755  , -10.34175   , -10.341745  ,\n",
      "       -10.34174   , -10.341734  , -10.341728  , -10.3417225 ,\n",
      "       -10.341716  , -10.341709  , -10.341702  , -10.341696  ,\n",
      "       -10.341688  , -10.341681  , -10.341672  , -10.341664  ,\n",
      "       -10.341656  , -10.341646  , -10.341637  , -10.341627  ,\n",
      "       -10.341617  , -10.341606  , -10.341595  , -10.341583  ,\n",
      "       -10.341572  , -10.341559  , -10.341546  , -10.341533  ,\n",
      "       -10.341518  , -10.341503  , -10.341488  , -10.341473  ,\n",
      "       -10.341455  , -10.341438  , -10.34142   , -10.341401  ,\n",
      "       -10.341382  , -10.341362  , -10.34134   , -10.341318  ,\n",
      "       -10.341295  , -10.341271  , -10.341247  , -10.341221  ,\n",
      "       -10.341193  , -10.341166  , -10.341136  , -10.341105  ,\n",
      "       -10.341074  , -10.341041  , -10.341006  , -10.34097   ,\n",
      "       -10.340933  , -10.340895  , -10.340854  , -10.340812  ,\n",
      "       -10.340768  , -10.340722  , -10.340674  , -10.340625  ,\n",
      "       -10.340573  , -10.340519  , -10.340463  , -10.3404045 ,\n",
      "       -10.340344  , -10.340281  , -10.340215  , -10.340146  ,\n",
      "       -10.340075  , -10.34      , -10.339923  , -10.339843  ,\n",
      "       -10.339759  , -10.339671  , -10.33958   , -10.339485  ,\n",
      "       -10.339386  , -10.339283  , -10.339176  , -10.339065  ,\n",
      "       -10.338948  , -10.338828  , -10.338701  , -10.338571  ,\n",
      "       -10.338434  , -10.338292  , -10.338143  , -10.33799   ,\n",
      "       -10.337829  , -10.337662  , -10.337487  , -10.337306  ,\n",
      "       -10.337117  , -10.336921  , -10.336716  , -10.336502  ,\n",
      "       -10.336281  , -10.336049  , -10.335809  , -10.335558  ,\n",
      "       -10.335297  , -10.335025  , -10.334742  , -10.334447  ,\n",
      "       -10.33414   , -10.333819  , -10.333487  , -10.333139  ,\n",
      "       -10.332779  , -10.332403  , -10.332011  , -10.331603  ,\n",
      "       -10.331179  , -10.330737  , -10.3302765 , -10.329797  ,\n",
      "       -10.329297  , -10.328777  , -10.328236  , -10.327672  ,\n",
      "       -10.3270855 , -10.326474  , -10.325837  , -10.325174  ,\n",
      "       -10.324484  , -10.323765  , -10.323016  , -10.322236  ,\n",
      "       -10.3214245 , -10.32058   , -10.319699  , -10.318782  ,\n",
      "       -10.317827  , -10.3168335 , -10.315798  , -10.31472   ,\n",
      "       -10.313598  , -10.312428  , -10.311212  , -10.309943  ,\n",
      "       -10.308623  , -10.307249  , -10.305818  , -10.304327  ,\n",
      "       -10.302774  , -10.301158  , -10.299475  , -10.297722  ,\n",
      "       -10.295897  , -10.293996  , -10.292017  , -10.289955  ,\n",
      "       -10.287809  , -10.285574  , -10.283246  , -10.280823  ,\n",
      "       -10.278298  , -10.27567   , -10.272933  , -10.270083  ,\n",
      "       -10.267116  , -10.264025  , -10.260806  , -10.257455  ,\n",
      "       -10.253964  , -10.250331  , -10.246546  , -10.242605  ,\n",
      "       -10.238502  , -10.234227  , -10.229777  , -10.225143  ,\n",
      "       -10.220317  , -10.215292  , -10.210059  , -10.20461   ,\n",
      "       -10.198935  , -10.193026  , -10.186872  , -10.180464  ,\n",
      "       -10.173791  , -10.166842  , -10.159606  , -10.152071  ,\n",
      "       -10.144224  , -10.136053  , -10.127544  , -10.118684  ,\n",
      "       -10.109457  , -10.099849  , -10.089843  , -10.079424  ,\n",
      "       -10.068573  , -10.057275  , -10.045509  , -10.033257  ,\n",
      "       -10.020498  , -10.007213  ,  -9.993377  ,  -9.97897   ,\n",
      "        -9.963966  ,  -9.948343  ,  -9.932075  ,  -9.9151325 ,\n",
      "        -9.8974905 ,  -9.87912   ,  -9.859988  ,  -9.840067  ,\n",
      "        -9.819322  ,  -9.797719  ,  -9.775223  ,  -9.751797  ,\n",
      "        -9.727402  ,  -9.701999  ,  -9.675546  ,  -9.647999  ,\n",
      "        -9.619313  ,  -9.589441  ,  -9.558335  ,  -9.525943  ,\n",
      "        -9.49221   ,  -9.457085  ,  -9.4205065 ,  -9.382416  ,\n",
      "        -9.342751  ,  -9.301446  ,  -9.258433  ,  -9.213642  ,\n",
      "        -9.167     ,  -9.118429  ,  -9.06785   ,  -9.015181  ,\n",
      "        -8.960333  ,  -8.903218  ,  -8.843742  ,  -8.781808  ,\n",
      "        -8.717313  ,  -8.650152  ,  -8.580214  ,  -8.507384  ,\n",
      "        -8.431544  ,  -8.352569  ,  -8.2703285 ,  -8.184689  ,\n",
      "        -8.095508  ,  -8.00264   ,  -7.9059334 ,  -7.805228  ,\n",
      "        -7.7003603 ,  -7.591157  ,  -7.477439  ,  -7.3590193 ,\n",
      "        -7.2357044 ,  -7.107291  ,  -6.97357   ,  -6.83432   ,\n",
      "        -6.689313  ,  -6.538312  ,  -6.381068  ,  -6.2173233 ,\n",
      "        -6.046809  ,  -5.8692455 ,  -5.6843414 ,  -5.4917936 ,\n",
      "        -5.2912855 ,  -5.082487  ,  -4.865058  ,  -4.6386395 ,\n",
      "        -4.4028606 ,  -4.1573343 ,  -3.9016573 ,  -3.635411  ,\n",
      "        -3.3581574 ,  -3.069442  ,  -2.7687905 ,  -2.4557097 ,\n",
      "        -2.129686  ,  -1.7901838 ,  -1.4366462 ,  -1.0684931 ,\n",
      "        -0.68511987,  -0.28589773,   0.12982893,   0.56274223,\n",
      "         1.0135527 ,   1.4830002 ,   1.9718552 ,   2.48092   ],\n",
      "      dtype=float32)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ybiao/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/ybiao/.local/lib/python3.6/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from roller import Roller\n",
    "from config import Params\n",
    "from wrapper_env import env_wrapper\n",
    "import gym\n",
    "\n",
    "from nn_architecures import network_builder\n",
    "from models import CategoricalModel, GaussianModel\n",
    "from policy import Policy\n",
    "from CREnv import CREnv\n",
    "\n",
    "\n",
    "# physical_devices = tf.config.list_physical_devices('GPU') \n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "params = Params()          # Get Configuration | HORIZON = Steps per epoch\n",
    "\n",
    "tf.random.set_seed(params.env.seed)                                     # Set Random Seeds for np and tf\n",
    "np.random.seed(params.env.seed)\n",
    "\n",
    "# env = create_batched_env(params.env.num_envs, params.env)               # Create Environment in multiprocessing mode\n",
    "# env = gym.make('CartPole-v0')\n",
    "env = CREnv()\n",
    "env = env_wrapper(env, params.env)\n",
    "\n",
    "network = network_builder(params.trainer.nn_architecure) \\\n",
    "    (hidden_sizes=params.policy.hidden_sizes, env_info=env.env_info)    # Build Neural Network with Forward Pass\n",
    "\n",
    "model = CategoricalModel if env.env_info.is_discrete else GaussianModel\n",
    "model = model(network=network, env_info=env.env_info)                   # Build Model for Discrete or Continuous Spaces\n",
    "\n",
    "roller = Roller(env, model, params.trainer.steps_per_epoch,\n",
    "                params.trainer.gamma, params.trainer.lam)               # Define Roller for genrating rollouts for training\n",
    "\n",
    "ppo = Policy(model=model)            # Define PPO Policy with combined loss\n",
    "rollouts, infos = roller.rollout()\n",
    "print(rollouts) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "level-ideal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "obs_shape = (4,)\n",
    "\n",
    "class ActorCritic(tf.keras.Model):\n",
    "    \"\"\"Combined actor-critic network.\"\"\"\n",
    "\n",
    "    def __init__(self, num_actions, hidden_units):\n",
    "        \"\"\"Initialize.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_actions = num_actions\n",
    "        self.hidden_sizes = hidden_units\n",
    "        \n",
    "        self.actor = self.mlp(output_size=self.n_actions)\n",
    "        self.critic = self.mlp(output_size=1)\n",
    "        self.actor.build(input_shape=(None,)+(4,))\n",
    "        self.critic.build(input_shape=(None,)+(4,))\n",
    "\n",
    "    # build mlp\n",
    "    def mlp(self, output_size=1, activation='relu', activation_output=None, kernel_init='glorot_uniform'):\n",
    "        model = tf.keras.Sequential()\n",
    "        for h in self.hidden_sizes:\n",
    "            model.add(tf.keras.layers.Dense(units=h, activation=activation, kernel_initializer=kernel_init, bias_initializer='zeros'))\n",
    "        model.add(tf.keras.layers.Dense(units=output_size, activation=activation_output))\n",
    "        return model\n",
    "    \n",
    "    def call(self, x):\n",
    "        return self.actor(x), self.critic(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "limiting-french",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(model, vec_obses, actions, advs, returns, logps):\n",
    "    \"\"\"\n",
    "        Update Policy and the Value Network\n",
    "        -----------------------------------\n",
    "            Inputs: obs, act, advantages, returns, logp-t\n",
    "            Returns: loss-pi, loss-entropy, approx-ent, kl, loss-v, loss-total\n",
    "    \"\"\"\n",
    "    nsample = 200\n",
    "    nbatch_train = nsample // 50\n",
    "    train_iters = 5\n",
    "    inds = np.arange(nsample)\n",
    "    loss_type = 'total_loss'\n",
    "    target_kl = 0.01\n",
    "    \n",
    "    np.random.shuffle(inds)\n",
    "    \n",
    "    for i in range(train_iters):\n",
    "        means = []\n",
    "        for start in range(0, nsample, nbatch_train):\n",
    "\n",
    "            end = start + nbatch_train\n",
    "            slices = inds[start:end]\n",
    "#             print(slices)\n",
    "            losses = train_one_step(model, vec_obses[slices],\n",
    "                                    actions[slices], \n",
    "                                    advs[slices], \n",
    "                                    logps[slices], \n",
    "                                    returns[slices],\n",
    "                                    loss_type)\n",
    "\n",
    "            means.append([losses['pi_loss'], \n",
    "                          losses['v_loss'], \n",
    "                          losses['entropy_loss'], \n",
    "                          losses['approx_ent'], \n",
    "                          losses['approx_kl']])\n",
    "\n",
    "        means = np.asarray(means)\n",
    "        means = np.mean(means, axis= 0)\n",
    "        \n",
    "        mean_losses = {'pi_loss': means[0], \n",
    "                    'v_loss': means[1],\n",
    "                    'entropy_loss': means[2], \n",
    "                    'approx_ent': means[3], \n",
    "                    'approx_kl': means[4]}\n",
    "        \n",
    "        if mean_losses['approx_kl'] > 1.5 * target_kl:\n",
    "            print(\"Early stopping at step %d due to reaching max kl.\" %i)\n",
    "            break\n",
    "    return mean_losses\n",
    "\n",
    "def loss(model, vec_obs, logp_old, act, adv, returns):\n",
    "    \n",
    "    action_dim = 2\n",
    "    clip_ratio = 0.2\n",
    "    ent_coef = 0.1\n",
    "    v_coef = 0.5\n",
    "    pi, value = model.call(vec_obs)\n",
    "\n",
    "    logp_all = tf.nn.log_softmax(pi)\n",
    "    one_hot = tf.one_hot(act, depth=action_dim)\n",
    "    logp = tf.reduce_sum(one_hot * logp_all, axis= -1) \n",
    "    \n",
    "    ratio = tf.exp(logp - logp_old)\n",
    "    min_adv = tf.where(adv > 0, (1 + clip_ratio) * adv, (1 - clip_ratio) * adv)\n",
    "\n",
    "    clipped_loss = -tf.reduce_mean(tf.math.minimum(ratio * adv, min_adv))\n",
    "        \n",
    "    # print(ratio, adv)\n",
    "    a0 = pi - tf.reduce_max(logits, axis= -1, keepdims=True)\n",
    "    exp_a0 = tf.exp(a0)\n",
    "    z0 = tf.reduce_sum(exp_a0, axis= -1, keepdims=True)\n",
    "    p0 = exp_a0 / z0\n",
    "    entropy = tf.reduce_sum(p0 * (tf.math.log(z0) - a0), axis= -1) \n",
    "    entropy_loss = -tf.reduce_mean(entropy)\n",
    "\n",
    "    pi_loss = clipped_loss + entropy_loss * ent_coef\n",
    "\n",
    "    approx_kl = tf.reduce_mean(logp_old - logp)\n",
    "    approx_ent = tf.reduce_mean(-logp)\n",
    "    \n",
    "    v_loss = 0.5 * tf.reduce_mean(tf.square(returns - value))\n",
    "    # v_loss = tf.reduce_mean(tf.square(returns - values))\n",
    "    total_loss = pi_loss + v_loss * v_coef\n",
    "\n",
    "    return {    'pi_loss': pi_loss, \n",
    "                'entropy_loss': entropy_loss, \n",
    "                'approx_ent': approx_ent, \n",
    "                'approx_kl': approx_kl, \n",
    "                'v_loss': v_loss, \n",
    "                'total_loss': total_loss}\n",
    "\n",
    "def train_one_step(model, vec_obs, act, adv, logp_old, returns, loss_type):\n",
    "    \n",
    "    clip_grads = 0.5\n",
    "    lr = 0.001\n",
    "    with tf.GradientTape() as tape:\n",
    "        losses = loss(model, vec_obs, logp_old, act, adv, returns)\n",
    "\n",
    "    if loss_type=='pi_loss':\n",
    "        trainable_variables = model.actor.trainable_variables            # take all trainable variables into account\n",
    "    elif loss_type=='v_loss':\n",
    "        trainable_variables = model.critic.trainable_variables\n",
    "    else:\n",
    "        trainable_variables = model.trainable_variables\n",
    "\n",
    "    grads = tape.gradient(losses[loss_type], trainable_variables)\n",
    "    grads, grad_norm = tf.clip_by_global_norm(grads, clip_grads)               # clip gradients for slight updates\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    optimizer.apply_gradients(zip(grads, trainable_variables))                 # Backprop gradients through network\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "available-grass",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 32)                160       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 1,282\n",
      "Trainable params: 1,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 32)                160       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,249\n",
      "Trainable params: 1,249\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "30.666666666666668\n",
      "{'pi_loss': -0.088376634, 'v_loss': 22.3279, 'entropy_loss': -0.6740826, 'approx_ent': 0.70184976, 'approx_kl': 0.01026049}\n",
      "41.5\n",
      "{'pi_loss': -0.08909188, 'v_loss': 34.14344, 'entropy_loss': -0.672047, 'approx_ent': 0.674752, 'approx_kl': 0.0014853776}\n",
      "28.285714285714285\n",
      "{'pi_loss': -0.0717878, 'v_loss': 46.663925, 'entropy_loss': -0.65808624, 'approx_ent': 0.66476, 'approx_kl': -0.0003125103}\n",
      "28.4\n",
      "{'pi_loss': -0.08082546, 'v_loss': 59.37434, 'entropy_loss': -0.6497669, 'approx_ent': 0.6627202, 'approx_kl': 0.01212462}\n",
      "39.8\n",
      "Early stopping at step 3 due to reaching max kl.\n",
      "{'pi_loss': -0.08094637, 'v_loss': 76.71812, 'entropy_loss': -0.62140316, 'approx_ent': 0.67748183, 'approx_kl': 0.015434567}\n",
      "87.5\n",
      "Early stopping at step 1 due to reaching max kl.\n",
      "{'pi_loss': -0.07774553, 'v_loss': 68.45545, 'entropy_loss': -0.6301038, 'approx_ent': 0.628623, 'approx_kl': 0.015931455}\n",
      "64.0\n",
      "{'pi_loss': -0.078968175, 'v_loss': 88.177315, 'entropy_loss': -0.5954622, 'approx_ent': 0.62304723, 'approx_kl': 0.007016661}\n",
      "54.666666666666664\n",
      "{'pi_loss': -0.06276395, 'v_loss': 105.80201, 'entropy_loss': -0.58630496, 'approx_ent': 0.5709141, 'approx_kl': 0.001103879}\n",
      "170.0\n",
      "Early stopping at step 4 due to reaching max kl.\n",
      "{'pi_loss': -0.07001035, 'v_loss': 74.77255, 'entropy_loss': -0.5564758, 'approx_ent': 0.547757, 'approx_kl': 0.015875766}\n",
      "70.5\n",
      "{'pi_loss': -0.04681869, 'v_loss': 118.51373, 'entropy_loss': -0.5067634, 'approx_ent': 0.47020608, 'approx_kl': -0.0035811432}\n",
      "57.0\n",
      "{'pi_loss': -0.074477345, 'v_loss': 119.14515, 'entropy_loss': -0.4876218, 'approx_ent': 0.58448964, 'approx_kl': 0.010802487}\n",
      "99.0\n",
      "{'pi_loss': -0.04784598, 'v_loss': 61.428814, 'entropy_loss': -0.5141269, 'approx_ent': 0.5293609, 'approx_kl': -0.0063517536}\n",
      "88.0\n",
      "Early stopping at step 4 due to reaching max kl.\n",
      "{'pi_loss': -0.05395666, 'v_loss': 123.58113, 'entropy_loss': -0.5509864, 'approx_ent': 0.5890287, 'approx_kl': 0.015254805}\n",
      "71.5\n",
      "{'pi_loss': -0.06425783, 'v_loss': 112.12768, 'entropy_loss': -0.5048454, 'approx_ent': 0.52533776, 'approx_kl': 0.00224607}\n",
      "114.0\n",
      "{'pi_loss': -0.027942479, 'v_loss': 79.039276, 'entropy_loss': -0.4331617, 'approx_ent': 0.45888406, 'approx_kl': 0.0034983149}\n",
      "71.0\n",
      "Early stopping at step 1 due to reaching max kl.\n",
      "{'pi_loss': -0.043788683, 'v_loss': 130.038, 'entropy_loss': -0.43307173, 'approx_ent': 0.4128354, 'approx_kl': 0.015321475}\n",
      "93.0\n",
      "{'pi_loss': -0.041355714, 'v_loss': 132.21515, 'entropy_loss': -0.48777658, 'approx_ent': 0.50061417, 'approx_kl': -0.0036693395}\n",
      "127.0\n",
      "{'pi_loss': -0.03800705, 'v_loss': 87.169815, 'entropy_loss': -0.44354695, 'approx_ent': 0.433134, 'approx_kl': -0.0019378662}\n",
      "168.0\n",
      "{'pi_loss': -0.049695607, 'v_loss': 103.322365, 'entropy_loss': -0.43400273, 'approx_ent': 0.4655679, 'approx_kl': 0.008335272}\n",
      "88.0\n",
      "{'pi_loss': -0.04553356, 'v_loss': 159.78668, 'entropy_loss': -0.42360166, 'approx_ent': 0.4215869, 'approx_kl': 0.007834942}\n",
      "83.0\n",
      "{'pi_loss': -0.03441671, 'v_loss': 135.0681, 'entropy_loss': -0.39876696, 'approx_ent': 0.4698845, 'approx_kl': 0.0008221038}\n",
      "93.0\n",
      "{'pi_loss': -0.030331908, 'v_loss': 87.24139, 'entropy_loss': -0.38743603, 'approx_ent': 0.4552486, 'approx_kl': 0.011491822}\n",
      "97.0\n",
      "{'pi_loss': 0.023102064, 'v_loss': 140.13272, 'entropy_loss': -0.39066228, 'approx_ent': 0.36309117, 'approx_kl': 0.0039683003}\n",
      "79.5\n",
      "{'pi_loss': -0.038162958, 'v_loss': 126.59113, 'entropy_loss': -0.41077083, 'approx_ent': 0.43130007, 'approx_kl': 0.008488446}\n",
      "89.0\n",
      "{'pi_loss': -0.024998862, 'v_loss': 116.441826, 'entropy_loss': -0.36840427, 'approx_ent': 0.3900435, 'approx_kl': 0.007863106}\n",
      "149.0\n",
      "{'pi_loss': 0.016642045, 'v_loss': 72.31968, 'entropy_loss': -0.38015, 'approx_ent': 0.35605672, 'approx_kl': 0.006701274}\n",
      "81.5\n",
      "{'pi_loss': -0.05067378, 'v_loss': 134.07799, 'entropy_loss': -0.37557837, 'approx_ent': 0.47637996, 'approx_kl': 0.009136553}\n",
      "72.0\n",
      "{'pi_loss': -0.00996746, 'v_loss': 124.92718, 'entropy_loss': -0.38787735, 'approx_ent': 0.34373882, 'approx_kl': 0.0071713896}\n",
      "95.5\n",
      "{'pi_loss': -0.053003997, 'v_loss': 124.11767, 'entropy_loss': -0.4183275, 'approx_ent': 0.44587296, 'approx_kl': 0.012594224}\n",
      "83.0\n",
      "{'pi_loss': -0.005028933, 'v_loss': 109.381325, 'entropy_loss': -0.36765358, 'approx_ent': 0.39767686, 'approx_kl': -0.0026136758}\n",
      "151.0\n",
      "{'pi_loss': -0.032585885, 'v_loss': 79.070274, 'entropy_loss': -0.34491795, 'approx_ent': 0.3748852, 'approx_kl': -0.0029963187}\n",
      "188.0\n",
      "{'pi_loss': -0.028235506, 'v_loss': 101.18063, 'entropy_loss': -0.35386524, 'approx_ent': 0.40216255, 'approx_kl': -0.0037434169}\n",
      "71.5\n",
      "{'pi_loss': -0.033072494, 'v_loss': 138.65846, 'entropy_loss': -0.39388934, 'approx_ent': 0.41274843, 'approx_kl': 0.0058587617}\n",
      "122.0\n",
      "{'pi_loss': -0.020746427, 'v_loss': 85.64554, 'entropy_loss': -0.3211637, 'approx_ent': 0.37894315, 'approx_kl': 0.00095634855}\n",
      "106.0\n",
      "Early stopping at step 2 due to reaching max kl.\n",
      "{'pi_loss': -0.03841368, 'v_loss': 90.29186, 'entropy_loss': -0.3687816, 'approx_ent': 0.28066078, 'approx_kl': 0.016351335}\n",
      "72.5\n",
      "Early stopping at step 2 due to reaching max kl.\n",
      "{'pi_loss': -0.06061006, 'v_loss': 154.25496, 'entropy_loss': -0.4363478, 'approx_ent': 0.43096778, 'approx_kl': 0.01947621}\n",
      "56.5\n",
      "Early stopping at step 1 due to reaching max kl.\n",
      "{'pi_loss': -0.027151894, 'v_loss': 133.72603, 'entropy_loss': -0.32150564, 'approx_ent': 0.37380135, 'approx_kl': 0.019780075}\n",
      "76.0\n",
      "Early stopping at step 0 due to reaching max kl.\n",
      "{'pi_loss': -0.043001886, 'v_loss': 117.190765, 'entropy_loss': -0.41597527, 'approx_ent': 0.42222515, 'approx_kl': 0.016945513}\n",
      "95.0\n",
      "{'pi_loss': -0.04407918, 'v_loss': 123.49769, 'entropy_loss': -0.445858, 'approx_ent': 0.43470347, 'approx_kl': 0.008793842}\n",
      "66.5\n",
      "{'pi_loss': -0.03612846, 'v_loss': 109.17502, 'entropy_loss': -0.40675625, 'approx_ent': 0.42210206, 'approx_kl': 0.010658772}\n",
      "85.0\n",
      "{'pi_loss': -0.047076922, 'v_loss': 114.20751, 'entropy_loss': -0.36465058, 'approx_ent': 0.3762839, 'approx_kl': 0.0028462892}\n",
      "80.0\n",
      "Early stopping at step 2 due to reaching max kl.\n",
      "{'pi_loss': -0.047987815, 'v_loss': 123.739716, 'entropy_loss': -0.42268112, 'approx_ent': 0.4520502, 'approx_kl': 0.015748274}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ybiao/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/ybiao/.local/lib/python3.8/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "{'pi_loss': -0.07557553, 'v_loss': 89.91515, 'entropy_loss': -0.4578965, 'approx_ent': 0.44915363, 'approx_kl': 0.0085265}\n",
      "182.0\n",
      "{'pi_loss': -0.047668207, 'v_loss': 105.62388, 'entropy_loss': -0.47353077, 'approx_ent': 0.44965374, 'approx_kl': 0.003544569}\n",
      "nan\n",
      "{'pi_loss': -0.045822755, 'v_loss': 129.1583, 'entropy_loss': -0.4524636, 'approx_ent': 0.43971333, 'approx_kl': -0.012743679}\n",
      "nan\n",
      "Early stopping at step 1 due to reaching max kl.\n",
      "{'pi_loss': -0.03446734, 'v_loss': 138.43916, 'entropy_loss': -0.4701195, 'approx_ent': 0.5128239, 'approx_kl': 0.01547514}\n",
      "165.0\n",
      "{'pi_loss': 0.009048533, 'v_loss': 128.13828, 'entropy_loss': -0.36919507, 'approx_ent': 0.4052278, 'approx_kl': -0.020322919}\n",
      "nan\n",
      "Early stopping at step 3 due to reaching max kl.\n",
      "{'pi_loss': -0.02972061, 'v_loss': 139.6433, 'entropy_loss': -0.40777463, 'approx_ent': 0.472226, 'approx_kl': 0.018590486}\n",
      "119.0\n",
      "Early stopping at step 1 due to reaching max kl.\n",
      "{'pi_loss': -0.036350645, 'v_loss': 142.42091, 'entropy_loss': -0.3401957, 'approx_ent': 0.33124408, 'approx_kl': 0.018415377}\n",
      "nan\n",
      "{'pi_loss': -0.07727472, 'v_loss': 134.50551, 'entropy_loss': -0.47563648, 'approx_ent': 0.4795825, 'approx_kl': 0.008121547}\n",
      "133.0\n",
      "{'pi_loss': -0.05419636, 'v_loss': 138.88403, 'entropy_loss': -0.35060146, 'approx_ent': 0.39887345, 'approx_kl': 0.012675268}\n",
      "108.0\n",
      "{'pi_loss': -0.0425218, 'v_loss': 136.80296, 'entropy_loss': -0.33854103, 'approx_ent': 0.31570825, 'approx_kl': -0.0002439224}\n",
      "132.0\n",
      "{'pi_loss': -0.026070904, 'v_loss': 146.59593, 'entropy_loss': -0.34393254, 'approx_ent': 0.3465356, 'approx_kl': 0.0055690943}\n",
      "116.0\n",
      "{'pi_loss': -0.0020725173, 'v_loss': 138.51, 'entropy_loss': -0.32706562, 'approx_ent': 0.33484972, 'approx_kl': -0.00038250154}\n",
      "159.0\n",
      "Early stopping at step 2 due to reaching max kl.\n",
      "{'pi_loss': -0.036850706, 'v_loss': 130.8248, 'entropy_loss': -0.2521771, 'approx_ent': 0.23687729, 'approx_kl': 0.020179117}\n",
      "98.0\n",
      "{'pi_loss': -0.046750836, 'v_loss': 140.1648, 'entropy_loss': -0.40927994, 'approx_ent': 0.41285715, 'approx_kl': 0.0048496854}\n",
      "117.0\n",
      "{'pi_loss': -0.03516118, 'v_loss': 132.57092, 'entropy_loss': -0.4072691, 'approx_ent': 0.41162148, 'approx_kl': 0.0066767526}\n",
      "99.5\n",
      "Early stopping at step 2 due to reaching max kl.\n",
      "{'pi_loss': -0.040601645, 'v_loss': 187.65376, 'entropy_loss': -0.3763924, 'approx_ent': 0.39338985, 'approx_kl': 0.018096259}\n",
      "78.0\n",
      "{'pi_loss': -0.048329182, 'v_loss': 154.71828, 'entropy_loss': -0.33734155, 'approx_ent': 0.29888058, 'approx_kl': 0.013233179}\n",
      "80.5\n",
      "Early stopping at step 2 due to reaching max kl.\n",
      "{'pi_loss': -0.010214036, 'v_loss': 130.16156, 'entropy_loss': -0.2801243, 'approx_ent': 0.2733103, 'approx_kl': 0.019615592}\n",
      "77.5\n",
      "Early stopping at step 3 due to reaching max kl.\n",
      "{'pi_loss': -0.043556098, 'v_loss': 126.09768, 'entropy_loss': -0.34510282, 'approx_ent': 0.28090104, 'approx_kl': 0.02098427}\n",
      "62.666666666666664\n",
      "Early stopping at step 4 due to reaching max kl.\n",
      "{'pi_loss': -0.041091662, 'v_loss': 112.40144, 'entropy_loss': -0.3593498, 'approx_ent': 0.3743086, 'approx_kl': 0.01565522}\n",
      "59.5\n",
      "Early stopping at step 1 due to reaching max kl.\n",
      "{'pi_loss': -0.046999812, 'v_loss': 85.97622, 'entropy_loss': -0.3812974, 'approx_ent': 0.3477963, 'approx_kl': 0.020128574}\n",
      "65.0\n",
      "Early stopping at step 0 due to reaching max kl.\n",
      "{'pi_loss': -0.033786103, 'v_loss': 103.8432, 'entropy_loss': -0.38248786, 'approx_ent': 0.42484012, 'approx_kl': 0.016820513}\n",
      "83.0\n",
      "Early stopping at step 2 due to reaching max kl.\n",
      "{'pi_loss': 0.0028874094, 'v_loss': 89.2469, 'entropy_loss': -0.28876722, 'approx_ent': 0.27965835, 'approx_kl': 0.024647808}\n",
      "63.333333333333336\n",
      "Early stopping at step 1 due to reaching max kl.\n",
      "{'pi_loss': -0.04354813, 'v_loss': 98.07207, 'entropy_loss': -0.35859576, 'approx_ent': 0.36256272, 'approx_kl': 0.017460883}\n",
      "66.0\n",
      "Early stopping at step 1 due to reaching max kl.\n",
      "{'pi_loss': -0.03807672, 'v_loss': 96.85661, 'entropy_loss': -0.3885473, 'approx_ent': 0.39402688, 'approx_kl': 0.021496536}\n",
      "62.333333333333336\n",
      "Early stopping at step 3 due to reaching max kl.\n",
      "{'pi_loss': -0.029738257, 'v_loss': 83.45369, 'entropy_loss': -0.34786364, 'approx_ent': 0.32051644, 'approx_kl': 0.016182065}\n",
      "86.5\n",
      "{'pi_loss': -0.03150102, 'v_loss': 87.84078, 'entropy_loss': -0.36259353, 'approx_ent': 0.36146766, 'approx_kl': 0.005899871}\n",
      "86.5\n",
      "Early stopping at step 4 due to reaching max kl.\n",
      "{'pi_loss': -0.036115, 'v_loss': 88.224464, 'entropy_loss': -0.31104895, 'approx_ent': 0.2753995, 'approx_kl': 0.01647159}\n",
      "66.5\n",
      "Early stopping at step 1 due to reaching max kl.\n",
      "{'pi_loss': -0.03965818, 'v_loss': 97.29748, 'entropy_loss': -0.26854628, 'approx_ent': 0.208448, 'approx_kl': 0.017768228}\n",
      "62.333333333333336\n",
      "{'pi_loss': -0.024899535, 'v_loss': 120.94292, 'entropy_loss': -0.34698528, 'approx_ent': 0.3303498, 'approx_kl': 8.118465e-05}\n",
      "54.666666666666664\n",
      "{'pi_loss': -0.036557484, 'v_loss': 96.911835, 'entropy_loss': -0.39283302, 'approx_ent': 0.39053056, 'approx_kl': -0.0013533726}\n",
      "51.0\n",
      "{'pi_loss': -0.04237617, 'v_loss': 92.47007, 'entropy_loss': -0.41948837, 'approx_ent': 0.39222622, 'approx_kl': 0.007599231}\n",
      "86.0\n",
      "Early stopping at step 0 due to reaching max kl.\n",
      "{'pi_loss': -0.0124270655, 'v_loss': 84.289665, 'entropy_loss': -0.32522556, 'approx_ent': 0.3257108, 'approx_kl': 0.016453661}\n",
      "51.0\n",
      "{'pi_loss': -0.05761366, 'v_loss': 91.161575, 'entropy_loss': -0.43064484, 'approx_ent': 0.43375483, 'approx_kl': 0.0045361747}\n",
      "174.0\n",
      "Early stopping at step 0 due to reaching max kl.\n",
      "{'pi_loss': -0.027173065, 'v_loss': 74.001915, 'entropy_loss': -0.24845442, 'approx_ent': 0.26266152, 'approx_kl': 0.018703572}\n",
      "80.0\n",
      "Early stopping at step 1 due to reaching max kl.\n",
      "{'pi_loss': -0.04332481, 'v_loss': 103.40426, 'entropy_loss': -0.34845144, 'approx_ent': 0.35973844, 'approx_kl': 0.019163972}\n",
      "140.0\n",
      "Early stopping at step 4 due to reaching max kl.\n",
      "{'pi_loss': 0.014304588, 'v_loss': 72.71862, 'entropy_loss': -0.33901933, 'approx_ent': 0.3026489, 'approx_kl': 0.023751045}\n",
      "46.0\n",
      "Early stopping at step 1 due to reaching max kl.\n",
      "{'pi_loss': -0.03260898, 'v_loss': 145.7863, 'entropy_loss': -0.28755233, 'approx_ent': 0.3395515, 'approx_kl': 0.020561635}\n",
      "67.5\n",
      "Early stopping at step 1 due to reaching max kl.\n",
      "{'pi_loss': -0.022869475, 'v_loss': 123.246925, 'entropy_loss': -0.3675571, 'approx_ent': 0.3949956, 'approx_kl': 0.02501147}\n",
      "85.0\n",
      "Early stopping at step 3 due to reaching max kl.\n",
      "{'pi_loss': -0.015692282, 'v_loss': 92.64493, 'entropy_loss': -0.300375, 'approx_ent': 0.3369488, 'approx_kl': 0.018323153}\n",
      "153.0\n",
      "{'pi_loss': 0.062225938, 'v_loss': 97.63687, 'entropy_loss': -0.2763861, 'approx_ent': 0.23510666, 'approx_kl': 0.010974187}\n",
      "95.0\n",
      "{'pi_loss': -0.046672866, 'v_loss': 110.31072, 'entropy_loss': -0.3850663, 'approx_ent': 0.3457388, 'approx_kl': -0.003022493}\n",
      "168.0\n",
      "{'pi_loss': 0.12974003, 'v_loss': 114.307556, 'entropy_loss': -0.32850587, 'approx_ent': 0.28549632, 'approx_kl': 0.0004879987}\n",
      "51.0\n",
      "{'pi_loss': -0.049918152, 'v_loss': 194.06796, 'entropy_loss': -0.4341861, 'approx_ent': 0.49071044, 'approx_kl': 0.00084805995}\n",
      "91.5\n",
      "Early stopping at step 0 due to reaching max kl.\n",
      "{'pi_loss': -0.011813278, 'v_loss': 136.63751, 'entropy_loss': -0.24683946, 'approx_ent': 0.30839413, 'approx_kl': 0.021631802}\n",
      "71.5\n",
      "{'pi_loss': -0.011745934, 'v_loss': 121.06407, 'entropy_loss': -0.24462985, 'approx_ent': 0.24234766, 'approx_kl': 0.008527755}\n",
      "69.5\n",
      "{'pi_loss': -0.026440103, 'v_loss': 114.68892, 'entropy_loss': -0.31881875, 'approx_ent': 0.34244522, 'approx_kl': -0.0009487007}\n",
      "53.0\n",
      "Early stopping at step 0 due to reaching max kl.\n",
      "{'pi_loss': -0.045485917, 'v_loss': 114.558525, 'entropy_loss': -0.4589361, 'approx_ent': 0.4832145, 'approx_kl': 0.035436455}\n",
      "42.0\n",
      "Early stopping at step 0 due to reaching max kl.\n",
      "{'pi_loss': -0.02790389, 'v_loss': 111.0844, 'entropy_loss': -0.23235217, 'approx_ent': 0.26871216, 'approx_kl': 0.017573968}\n",
      "124.0\n",
      "Early stopping at step 1 due to reaching max kl.\n",
      "{'pi_loss': -0.011889695, 'v_loss': 87.0795, 'entropy_loss': -0.30760247, 'approx_ent': 0.30443063, 'approx_kl': 0.016248144}\n",
      "98.0\n",
      "Early stopping at step 1 due to reaching max kl.\n",
      "{'pi_loss': -0.039233636, 'v_loss': 141.98164, 'entropy_loss': -0.40375277, 'approx_ent': 0.35582736, 'approx_kl': 0.016781623}\n",
      "92.5\n",
      "Early stopping at step 1 due to reaching max kl.\n",
      "{'pi_loss': -0.040167846, 'v_loss': 137.91936, 'entropy_loss': -0.31602672, 'approx_ent': 0.36418006, 'approx_kl': 0.03995162}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153.0\n",
      "Early stopping at step 2 due to reaching max kl.\n",
      "{'pi_loss': -0.036773633, 'v_loss': 83.35416, 'entropy_loss': -0.4216482, 'approx_ent': 0.42804414, 'approx_kl': 0.018318191}\n",
      "nan\n",
      "Early stopping at step 1 due to reaching max kl.\n",
      "{'pi_loss': 0.009763537, 'v_loss': 101.32596, 'entropy_loss': -0.26492032, 'approx_ent': 0.22885679, 'approx_kl': 0.02873401}\n",
      "63.0\n",
      "{'pi_loss': -0.040155396, 'v_loss': 106.19511, 'entropy_loss': -0.3360642, 'approx_ent': 0.35992578, 'approx_kl': 0.009820402}\n",
      "181.0\n",
      "Early stopping at step 0 due to reaching max kl.\n",
      "{'pi_loss': -0.011881041, 'v_loss': 118.299324, 'entropy_loss': -0.24466623, 'approx_ent': 0.29108226, 'approx_kl': 0.015258235}\n",
      "53.0\n",
      "{'pi_loss': -0.008105241, 'v_loss': 128.22037, 'entropy_loss': -0.33152694, 'approx_ent': 0.36016247, 'approx_kl': 0.0036915208}\n",
      "65.5\n",
      "Early stopping at step 1 due to reaching max kl.\n",
      "{'pi_loss': -0.053974308, 'v_loss': 184.5058, 'entropy_loss': -0.34035072, 'approx_ent': 0.286903, 'approx_kl': 0.015848735}\n",
      "83.5\n",
      "{'pi_loss': -0.052484218, 'v_loss': 131.23174, 'entropy_loss': -0.32035038, 'approx_ent': 0.35294855, 'approx_kl': 0.0074789347}\n",
      "85.0\n",
      "Early stopping at step 1 due to reaching max kl.\n",
      "{'pi_loss': -0.044046715, 'v_loss': 138.24066, 'entropy_loss': -0.34268227, 'approx_ent': 0.32160637, 'approx_kl': 0.016427293}\n",
      "46.0\n",
      "Early stopping at step 1 due to reaching max kl.\n",
      "{'pi_loss': -0.053249873, 'v_loss': 96.4813, 'entropy_loss': -0.2824502, 'approx_ent': 0.32777035, 'approx_kl': 0.04019857}\n",
      "55.0\n",
      "Early stopping at step 1 due to reaching max kl.\n",
      "{'pi_loss': 0.0202268, 'v_loss': 109.69283, 'entropy_loss': -0.2309984, 'approx_ent': 0.19421116, 'approx_kl': 0.028797548}\n",
      "45.25\n",
      "Early stopping at step 2 due to reaching max kl.\n",
      "{'pi_loss': -0.031543992, 'v_loss': 175.48402, 'entropy_loss': -0.36627018, 'approx_ent': 0.4644606, 'approx_kl': 0.021233298}\n",
      "41.25\n",
      "Early stopping at step 0 due to reaching max kl.\n",
      "{'pi_loss': -0.048226796, 'v_loss': 118.012566, 'entropy_loss': -0.4695263, 'approx_ent': 0.50512004, 'approx_kl': 0.025852187}\n",
      "88.0\n",
      "{'pi_loss': -0.04322299, 'v_loss': 85.15175, 'entropy_loss': -0.35255593, 'approx_ent': 0.41514727, 'approx_kl': 0.0046671103}\n",
      "82.5\n",
      "Early stopping at step 1 due to reaching max kl.\n",
      "{'pi_loss': -0.02375021, 'v_loss': 117.51818, 'entropy_loss': -0.3577118, 'approx_ent': 0.36741152, 'approx_kl': 0.018716712}\n",
      "38.333333333333336\n",
      "Early stopping at step 0 due to reaching max kl.\n",
      "{'pi_loss': -0.029590497, 'v_loss': 117.317, 'entropy_loss': -0.27332023, 'approx_ent': 0.35955676, 'approx_kl': 0.023529}\n",
      "161.0\n",
      "Early stopping at step 0 due to reaching max kl.\n",
      "{'pi_loss': -0.016376851, 'v_loss': 78.42705, 'entropy_loss': -0.35658696, 'approx_ent': 0.4208172, 'approx_kl': 0.017687863}\n",
      "nan\n",
      "Early stopping at step 0 due to reaching max kl.\n",
      "{'pi_loss': -0.009579, 'v_loss': 110.2595, 'entropy_loss': -0.21446018, 'approx_ent': 0.23137401, 'approx_kl': 0.024408758}\n",
      "108.0\n",
      "Early stopping at step 2 due to reaching max kl.\n",
      "{'pi_loss': -0.015129965, 'v_loss': 77.24371, 'entropy_loss': -0.39172196, 'approx_ent': 0.44053513, 'approx_kl': 0.022131018}\n",
      "61.0\n",
      "Early stopping at step 1 due to reaching max kl.\n",
      "{'pi_loss': -0.0299266, 'v_loss': 133.21942, 'entropy_loss': -0.30686384, 'approx_ent': 0.30363262, 'approx_kl': 0.01711557}\n",
      "60.666666666666664\n",
      "Early stopping at step 0 due to reaching max kl.\n",
      "{'pi_loss': -0.03153163, 'v_loss': 156.30128, 'entropy_loss': -0.38655224, 'approx_ent': 0.3265642, 'approx_kl': 0.021501057}\n",
      "83.5\n",
      "{'pi_loss': -0.052896425, 'v_loss': 106.42625, 'entropy_loss': -0.3821249, 'approx_ent': 0.4148096, 'approx_kl': 0.008870448}\n",
      "83.5\n",
      "Early stopping at step 1 due to reaching max kl.\n",
      "{'pi_loss': -0.04023501, 'v_loss': 104.75318, 'entropy_loss': -0.30616534, 'approx_ent': 0.30988732, 'approx_kl': 0.018971836}\n",
      "48.5\n",
      "{'pi_loss': -0.064295866, 'v_loss': 107.48828, 'entropy_loss': -0.48281774, 'approx_ent': 0.47421005, 'approx_kl': 0.014271327}\n",
      "nan\n",
      "{'pi_loss': -0.034801796, 'v_loss': 60.313087, 'entropy_loss': -0.34664875, 'approx_ent': 0.30872068, 'approx_kl': -0.007826013}\n",
      "59.5\n",
      "{'pi_loss': 0.0049311286, 'v_loss': 132.11436, 'entropy_loss': -0.41559222, 'approx_ent': 0.42291677, 'approx_kl': -0.0024325128}\n",
      "46.0\n",
      "{'pi_loss': -0.05582113, 'v_loss': 96.41648, 'entropy_loss': -0.35570607, 'approx_ent': 0.35462886, 'approx_kl': 0.007895997}\n",
      "132.0\n",
      "Early stopping at step 2 due to reaching max kl.\n",
      "{'pi_loss': -0.012900595, 'v_loss': 95.7484, 'entropy_loss': -0.33762845, 'approx_ent': 0.33235177, 'approx_kl': 0.017663078}\n",
      "195.0\n",
      "{'pi_loss': -0.044264384, 'v_loss': 115.57513, 'entropy_loss': -0.39815655, 'approx_ent': 0.40509942, 'approx_kl': 0.0025811493}\n",
      "nan\n",
      "Early stopping at step 1 due to reaching max kl.\n",
      "{'pi_loss': 0.0039929375, 'v_loss': 125.96544, 'entropy_loss': -0.27004156, 'approx_ent': 0.33557105, 'approx_kl': 0.016907685}\n",
      "nan\n",
      "Early stopping at step 2 due to reaching max kl.\n",
      "{'pi_loss': -0.042003088, 'v_loss': 136.58209, 'entropy_loss': -0.33537853, 'approx_ent': 0.25520024, 'approx_kl': 0.03236502}\n",
      "97.0\n",
      "Early stopping at step 0 due to reaching max kl.\n",
      "{'pi_loss': -0.001737973, 'v_loss': 210.7661, 'entropy_loss': -0.36307687, 'approx_ent': 0.38359672, 'approx_kl': 0.01761544}\n",
      "142.0\n",
      "Early stopping at step 0 due to reaching max kl.\n",
      "{'pi_loss': -0.0302471, 'v_loss': 111.28264, 'entropy_loss': -0.27662867, 'approx_ent': 0.26779786, 'approx_kl': 0.017426537}\n",
      "119.0\n",
      "{'pi_loss': 0.040987853, 'v_loss': 111.55046, 'entropy_loss': -0.36390963, 'approx_ent': 0.36161312, 'approx_kl': -0.018045299}\n",
      "97.0\n",
      "{'pi_loss': 0.011682902, 'v_loss': 116.00146, 'entropy_loss': -0.3837543, 'approx_ent': 0.3985266, 'approx_kl': -0.008820975}\n",
      "nan\n",
      "{'pi_loss': -0.024790488, 'v_loss': 122.39336, 'entropy_loss': -0.30284125, 'approx_ent': 0.3372041, 'approx_kl': -0.0053131087}\n",
      "124.0\n",
      "Early stopping at step 1 due to reaching max kl.\n",
      "{'pi_loss': -0.01429939, 'v_loss': 120.88561, 'entropy_loss': -0.29880983, 'approx_ent': 0.31275868, 'approx_kl': 0.016171345}\n",
      "nan\n",
      "{'pi_loss': -0.038869362, 'v_loss': 135.31914, 'entropy_loss': -0.40702164, 'approx_ent': 0.37471467, 'approx_kl': 0.0046336264}\n",
      "nan\n",
      "{'pi_loss': -0.04324644, 'v_loss': 139.6172, 'entropy_loss': -0.48650962, 'approx_ent': 0.4081917, 'approx_kl': -0.00887645}\n",
      "93.0\n",
      "Early stopping at step 0 due to reaching max kl.\n",
      "{'pi_loss': -0.032576993, 'v_loss': 134.56754, 'entropy_loss': -0.34318203, 'approx_ent': 0.39292535, 'approx_kl': 0.029447855}\n",
      "nan\n",
      "Early stopping at step 0 due to reaching max kl.\n",
      "{'pi_loss': -0.016679525, 'v_loss': 140.57825, 'entropy_loss': -0.46182087, 'approx_ent': 0.50465167, 'approx_kl': 0.021177389}\n",
      "nan\n",
      "{'pi_loss': -0.035254475, 'v_loss': 149.89589, 'entropy_loss': -0.47479138, 'approx_ent': 0.4952281, 'approx_kl': 0.009594181}\n",
      "nan\n",
      "{'pi_loss': -0.03967726, 'v_loss': 153.13792, 'entropy_loss': -0.46014988, 'approx_ent': 0.39656457, 'approx_kl': -0.017431503}\n",
      "nan\n",
      "{'pi_loss': -0.039183486, 'v_loss': 141.31589, 'entropy_loss': -0.42940834, 'approx_ent': 0.39844376, 'approx_kl': -0.0076876176}\n",
      "89.0\n",
      "Early stopping at step 1 due to reaching max kl.\n",
      "{'pi_loss': -0.034127768, 'v_loss': 193.7165, 'entropy_loss': -0.34351045, 'approx_ent': 0.36914158, 'approx_kl': 0.03083276}\n",
      "88.5\n",
      "Early stopping at step 0 due to reaching max kl.\n",
      "{'pi_loss': -0.015448213, 'v_loss': 159.28087, 'entropy_loss': -0.3266988, 'approx_ent': 0.34089053, 'approx_kl': 0.015862413}\n",
      "nan\n",
      "Early stopping at step 1 due to reaching max kl.\n",
      "{'pi_loss': -0.036788195, 'v_loss': 103.074196, 'entropy_loss': -0.30875298, 'approx_ent': 0.3470304, 'approx_kl': 0.019743726}\n",
      "94.0\n",
      "{'pi_loss': -0.014365592, 'v_loss': 152.36493, 'entropy_loss': -0.34709606, 'approx_ent': 0.36568883, 'approx_kl': 0.013310481}\n",
      "66.0\n",
      "Early stopping at step 1 due to reaching max kl.\n",
      "{'pi_loss': -0.031595096, 'v_loss': 144.3575, 'entropy_loss': -0.38824886, 'approx_ent': 0.43720788, 'approx_kl': 0.017258918}\n",
      "63.333333333333336\n",
      "Early stopping at step 1 due to reaching max kl.\n",
      "{'pi_loss': -0.022484405, 'v_loss': 118.23392, 'entropy_loss': -0.31657815, 'approx_ent': 0.32762143, 'approx_kl': 0.031980503}\n",
      "93.0\n",
      "{'pi_loss': -0.045898795, 'v_loss': 90.95818, 'entropy_loss': -0.481576, 'approx_ent': 0.5369726, 'approx_kl': 0.010669142}\n",
      "82.5\n",
      "Early stopping at step 1 due to reaching max kl.\n",
      "{'pi_loss': -0.04347459, 'v_loss': 91.80371, 'entropy_loss': -0.32518846, 'approx_ent': 0.330783, 'approx_kl': 0.019181924}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.0\n",
      "{'pi_loss': -0.029353423, 'v_loss': 114.30518, 'entropy_loss': -0.3070734, 'approx_ent': 0.31797042, 'approx_kl': 0.0100741815}\n",
      "46.0\n",
      "{'pi_loss': -0.014050753, 'v_loss': 96.41682, 'entropy_loss': -0.3758054, 'approx_ent': 0.3662269, 'approx_kl': 0.007953371}\n",
      "94.5\n",
      "Early stopping at step 1 due to reaching max kl.\n",
      "{'pi_loss': 0.05116224, 'v_loss': 77.416336, 'entropy_loss': -0.22766054, 'approx_ent': 0.28449157, 'approx_kl': 0.025700722}\n",
      "42.0\n",
      "Early stopping at step 0 due to reaching max kl.\n",
      "{'pi_loss': -0.073743746, 'v_loss': 85.996574, 'entropy_loss': -0.37066573, 'approx_ent': 0.32760483, 'approx_kl': 0.045096647}\n",
      "62.0\n",
      "Early stopping at step 0 due to reaching max kl.\n",
      "{'pi_loss': -0.03739102, 'v_loss': 78.065544, 'entropy_loss': -0.30319777, 'approx_ent': 0.30952194, 'approx_kl': 0.020434808}\n",
      "61.333333333333336\n",
      "Early stopping at step 1 due to reaching max kl.\n",
      "{'pi_loss': -0.028337132, 'v_loss': 88.75558, 'entropy_loss': -0.36287895, 'approx_ent': 0.38261548, 'approx_kl': 0.04549793}\n",
      "57.0\n",
      "{'pi_loss': 0.062619194, 'v_loss': 87.16682, 'entropy_loss': -0.32168254, 'approx_ent': 0.32464626, 'approx_kl': 0.0021879654}\n",
      "57.0\n",
      "Early stopping at step 0 due to reaching max kl.\n",
      "{'pi_loss': -0.0056909425, 'v_loss': 87.27857, 'entropy_loss': -0.40111226, 'approx_ent': 0.44421467, 'approx_kl': 0.030838506}\n",
      "76.5\n",
      "Early stopping at step 2 due to reaching max kl.\n",
      "{'pi_loss': 0.029523414, 'v_loss': 75.20492, 'entropy_loss': -0.43473437, 'approx_ent': 0.45463893, 'approx_kl': 0.02049453}\n",
      "55.333333333333336\n",
      "{'pi_loss': -0.03944857, 'v_loss': 91.45978, 'entropy_loss': -0.3804966, 'approx_ent': 0.34893197, 'approx_kl': -0.006183857}\n",
      "nan\n",
      "{'pi_loss': -0.048212595, 'v_loss': 66.15441, 'entropy_loss': -0.39089036, 'approx_ent': 0.4155419, 'approx_kl': 0.00646933}\n",
      "93.0\n",
      "Early stopping at step 0 due to reaching max kl.\n",
      "{'pi_loss': -0.031226482, 'v_loss': 84.719376, 'entropy_loss': -0.2968434, 'approx_ent': 0.31407243, 'approx_kl': 0.016755734}\n",
      "21.0\n",
      "Early stopping at step 1 due to reaching max kl.\n",
      "{'pi_loss': -0.036891576, 'v_loss': 95.45625, 'entropy_loss': -0.35317054, 'approx_ent': 0.33792642, 'approx_kl': 0.01915258}\n",
      "nan\n",
      "Early stopping at step 0 due to reaching max kl.\n",
      "{'pi_loss': -0.002301417, 'v_loss': 129.99245, 'entropy_loss': -0.26714677, 'approx_ent': 0.28899512, 'approx_kl': 0.017762454}\n",
      "120.0\n",
      "Early stopping at step 0 due to reaching max kl.\n",
      "{'pi_loss': -0.029871326, 'v_loss': 123.77958, 'entropy_loss': -0.2924292, 'approx_ent': 0.26856315, 'approx_kl': 0.015520027}\n",
      "149.0\n",
      "Early stopping at step 2 due to reaching max kl.\n",
      "{'pi_loss': 0.052321397, 'v_loss': 116.26071, 'entropy_loss': -0.2826086, 'approx_ent': 0.28609154, 'approx_kl': 0.022947531}\n",
      "nan\n",
      "Early stopping at step 3 due to reaching max kl.\n",
      "{'pi_loss': -0.024779782, 'v_loss': 134.3407, 'entropy_loss': -0.2494534, 'approx_ent': 0.3045226, 'approx_kl': 0.021605723}\n",
      "nan\n",
      "Early stopping at step 1 due to reaching max kl.\n",
      "{'pi_loss': -0.048032995, 'v_loss': 143.21695, 'entropy_loss': -0.34769875, 'approx_ent': 0.32102287, 'approx_kl': 0.023660557}\n",
      "118.0\n",
      "{'pi_loss': -0.016558515, 'v_loss': 132.73718, 'entropy_loss': -0.24230312, 'approx_ent': 0.24823003, 'approx_kl': 0.0045837467}\n",
      "nan\n",
      "Early stopping at step 4 due to reaching max kl.\n",
      "{'pi_loss': -0.013726065, 'v_loss': 132.73936, 'entropy_loss': -0.22641638, 'approx_ent': 0.2880606, 'approx_kl': 0.016562425}\n",
      "nan\n",
      "{'pi_loss': -0.035755068, 'v_loss': 136.13081, 'entropy_loss': -0.29356995, 'approx_ent': 0.25417072, 'approx_kl': 0.0043359143}\n",
      "nan\n",
      "{'pi_loss': -0.047187556, 'v_loss': 136.68608, 'entropy_loss': -0.2997508, 'approx_ent': 0.33639374, 'approx_kl': -0.00068029755}\n",
      "168.0\n",
      "Early stopping at step 1 due to reaching max kl.\n",
      "{'pi_loss': -0.046426516, 'v_loss': 138.92778, 'entropy_loss': -0.3360955, 'approx_ent': 0.4266897, 'approx_kl': 0.015157156}\n",
      "142.0\n",
      "Early stopping at step 3 due to reaching max kl.\n",
      "{'pi_loss': 0.04574029, 'v_loss': 137.61124, 'entropy_loss': -0.31279552, 'approx_ent': 0.34909523, 'approx_kl': 0.021868128}\n",
      "91.0\n",
      "{'pi_loss': -0.027190631, 'v_loss': 138.03387, 'entropy_loss': -0.31882745, 'approx_ent': 0.32473454, 'approx_kl': 0.0050718654}\n",
      "120.0\n",
      "Early stopping at step 3 due to reaching max kl.\n",
      "{'pi_loss': 0.00062535226, 'v_loss': 132.15686, 'entropy_loss': -0.2986546, 'approx_ent': 0.2568107, 'approx_kl': 0.019023113}\n",
      "95.0\n",
      "Early stopping at step 0 due to reaching max kl.\n",
      "{'pi_loss': -0.04036157, 'v_loss': 218.30336, 'entropy_loss': -0.36012843, 'approx_ent': 0.4059502, 'approx_kl': 0.0151339825}\n",
      "132.0\n",
      "{'pi_loss': 0.004178267, 'v_loss': 119.11219, 'entropy_loss': -0.31050617, 'approx_ent': 0.30295283, 'approx_kl': 0.009642151}\n",
      "85.0\n",
      "{'pi_loss': 0.015848072, 'v_loss': 173.40375, 'entropy_loss': -0.3200863, 'approx_ent': 0.33241844, 'approx_kl': 0.008912594}\n",
      "102.0\n",
      "Early stopping at step 3 due to reaching max kl.\n",
      "{'pi_loss': 0.0012576139, 'v_loss': 98.96401, 'entropy_loss': -0.38395986, 'approx_ent': 0.40181756, 'approx_kl': 0.02926226}\n",
      "107.0\n",
      "Early stopping at step 2 due to reaching max kl.\n",
      "{'pi_loss': -0.011619322, 'v_loss': 108.666306, 'entropy_loss': -0.3086851, 'approx_ent': 0.24833961, 'approx_kl': 0.015752088}\n",
      "68.5\n",
      "Early stopping at step 1 due to reaching max kl.\n",
      "{'pi_loss': -0.033007983, 'v_loss': 180.02481, 'entropy_loss': -0.28282288, 'approx_ent': 0.34639075, 'approx_kl': 0.015293778}\n",
      "36.75\n",
      "Early stopping at step 0 due to reaching max kl.\n",
      "{'pi_loss': -0.045038935, 'v_loss': 228.83086, 'entropy_loss': -0.27239975, 'approx_ent': 0.33287317, 'approx_kl': 0.037874565}\n",
      "92.0\n",
      "Early stopping at step 4 due to reaching max kl.\n",
      "{'pi_loss': -0.027254047, 'v_loss': 110.76193, 'entropy_loss': -0.34429005, 'approx_ent': 0.34627354, 'approx_kl': 0.016841197}\n",
      "164.0\n",
      "Early stopping at step 4 due to reaching max kl.\n",
      "{'pi_loss': -0.035501774, 'v_loss': 83.17751, 'entropy_loss': -0.267223, 'approx_ent': 0.2846245, 'approx_kl': 0.015655965}\n",
      "78.0\n",
      "Early stopping at step 1 due to reaching max kl.\n",
      "{'pi_loss': -0.03560643, 'v_loss': 98.38431, 'entropy_loss': -0.28376445, 'approx_ent': 0.28826082, 'approx_kl': 0.017905237}\n",
      "46.0\n",
      "Early stopping at step 3 due to reaching max kl.\n",
      "{'pi_loss': 0.007780842, 'v_loss': 179.50577, 'entropy_loss': -0.31478414, 'approx_ent': 0.23361504, 'approx_kl': 0.018619586}\n",
      "34.2\n",
      "Early stopping at step 0 due to reaching max kl.\n",
      "{'pi_loss': 0.14312215, 'v_loss': 165.77505, 'entropy_loss': -0.32725868, 'approx_ent': 0.40061483, 'approx_kl': 0.04492508}\n",
      "21.0\n",
      "Early stopping at step 0 due to reaching max kl.\n",
      "{'pi_loss': -0.050378304, 'v_loss': 77.3177, 'entropy_loss': -0.25398907, 'approx_ent': 0.2798307, 'approx_kl': 0.019580664}\n",
      "44.75\n",
      "Early stopping at step 0 due to reaching max kl.\n",
      "{'pi_loss': 0.003199128, 'v_loss': 138.7285, 'entropy_loss': -0.42624554, 'approx_ent': 0.46174258, 'approx_kl': 0.016837016}\n",
      "41.75\n",
      "Early stopping at step 0 due to reaching max kl.\n",
      "{'pi_loss': -0.04516388, 'v_loss': 95.51884, 'entropy_loss': -0.429907, 'approx_ent': 0.45980442, 'approx_kl': 0.01787182}\n",
      "76.5\n",
      "Early stopping at step 2 due to reaching max kl.\n",
      "{'pi_loss': -0.04035858, 'v_loss': 69.03588, 'entropy_loss': -0.30983397, 'approx_ent': 0.3727372, 'approx_kl': 0.017534735}\n",
      "48.666666666666664\n",
      "Early stopping at step 2 due to reaching max kl.\n",
      "{'pi_loss': -0.02423977, 'v_loss': 90.07325, 'entropy_loss': -0.3480438, 'approx_ent': 0.37058473, 'approx_kl': 0.024165144}\n",
      "56.333333333333336\n",
      "Early stopping at step 2 due to reaching max kl.\n",
      "{'pi_loss': -0.009161956, 'v_loss': 84.950165, 'entropy_loss': -0.35836184, 'approx_ent': 0.4397589, 'approx_kl': 0.015977308}\n",
      "47.0\n",
      "{'pi_loss': -0.007357165, 'v_loss': 61.887196, 'entropy_loss': -0.28512198, 'approx_ent': 0.2792841, 'approx_kl': -0.009693777}\n",
      "69.0\n",
      "Early stopping at step 0 due to reaching max kl.\n",
      "{'pi_loss': 0.0013377798, 'v_loss': 81.68945, 'entropy_loss': -0.28611708, 'approx_ent': 0.2562595, 'approx_kl': 0.017763298}\n",
      "63.666666666666664\n",
      "Early stopping at step 0 due to reaching max kl.\n",
      "{'pi_loss': -0.03217247, 'v_loss': 156.01222, 'entropy_loss': -0.31584966, 'approx_ent': 0.3572874, 'approx_kl': 0.029974174}\n",
      "59.5\n",
      "Early stopping at step 3 due to reaching max kl.\n",
      "{'pi_loss': -0.008164354, 'v_loss': 116.01436, 'entropy_loss': -0.3574039, 'approx_ent': 0.29954296, 'approx_kl': 0.01629927}\n",
      "120.0\n",
      "{'pi_loss': -0.011178288, 'v_loss': 75.963524, 'entropy_loss': -0.29710928, 'approx_ent': 0.29828718, 'approx_kl': 0.006129738}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135.0\n",
      "Early stopping at step 0 due to reaching max kl.\n",
      "{'pi_loss': -0.041040443, 'v_loss': 93.48047, 'entropy_loss': -0.32888716, 'approx_ent': 0.36445618, 'approx_kl': 0.029592719}\n",
      "58.0\n",
      "Early stopping at step 0 due to reaching max kl.\n",
      "{'pi_loss': -0.033125784, 'v_loss': 205.2362, 'entropy_loss': -0.25919732, 'approx_ent': 0.3886633, 'approx_kl': 0.07225399}\n",
      "150.0\n",
      "Early stopping at step 4 due to reaching max kl.\n",
      "{'pi_loss': -0.040965978, 'v_loss': 85.480865, 'entropy_loss': -0.31979764, 'approx_ent': 0.3865658, 'approx_kl': 0.026944647}\n",
      "67.0\n",
      "Early stopping at step 0 due to reaching max kl.\n",
      "{'pi_loss': -0.036504317, 'v_loss': 151.04198, 'entropy_loss': -0.29212087, 'approx_ent': 0.30603063, 'approx_kl': 0.022130586}\n",
      "51.0\n",
      "{'pi_loss': -0.016319888, 'v_loss': 93.607414, 'entropy_loss': -0.3489653, 'approx_ent': 0.3818686, 'approx_kl': -0.0060985005}\n",
      "142.0\n",
      "Early stopping at step 3 due to reaching max kl.\n",
      "{'pi_loss': -0.021982225, 'v_loss': 98.77708, 'entropy_loss': -0.30000085, 'approx_ent': 0.26633313, 'approx_kl': 0.022213459}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "\n",
    "# set params\n",
    "seed = 10000\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "n_actions = env.action_space.n  # 2\n",
    "hidden_units = (32,32)\n",
    "\n",
    "model = ActorCritic(n_actions, hidden_units)\n",
    "model.actor.summary()\n",
    "model.critic.summary()\n",
    "\n",
    "gamma = 0.99\n",
    "lam = 0.97\n",
    "\n",
    "for epoch in range(200):\n",
    "    # rollout\n",
    "    obs = env.reset()\n",
    "\n",
    "    num_steps = 200\n",
    "\n",
    "    vec_obses = []\n",
    "    rews = []\n",
    "    dones = []\n",
    "    actions = []\n",
    "    values = []\n",
    "    logps = []\n",
    "    ep_rews = []\n",
    "    ep_lens = []\n",
    "\n",
    "    # print(np.expand_dims(obs, axis=0).shape)\n",
    "    # print(model.call(np.expand_dims(obs, axis=0)))\n",
    "    ep_len = 0\n",
    "    ep_rew = 0\n",
    "    done_t = False\n",
    "    for step in range(num_steps):\n",
    "\n",
    "        if done_t:\n",
    "            obs = env.reset()\n",
    "            ep_lens.append(ep_len)\n",
    "            ep_rews.append(ep_rew)\n",
    "            ep_len = 0\n",
    "            ep_rew = 0\n",
    "\n",
    "        logits, value = model.call(np.expand_dims(obs, axis=0))\n",
    "        action_t = tf.squeeze(tf.random.categorical(logits, 1), axis=-1)\n",
    "        logp_all = tf.nn.log_softmax(logits)\n",
    "        one_hot = tf.one_hot(action_t, depth=n_actions)\n",
    "        logp_t = tf.reduce_sum( one_hot * logp_all, axis= -1)\n",
    "\n",
    "        vec_obses.append(obs)\n",
    "        dones.append(done_t)\n",
    "        actions.append(action_t)\n",
    "        values.append(value)\n",
    "        logps.append(logp_t)\n",
    "\n",
    "        obs, rew, done_t, infos = env.step(action_t.numpy()[0])\n",
    "\n",
    "        rews.append(rew)\n",
    "\n",
    "        ep_len += 1\n",
    "        ep_rew += rew\n",
    "        \n",
    "    print(np.mean(ep_lens))\n",
    "\n",
    "    # ep_lens.append(ep_len)\n",
    "    # ep_rews.append(ep_rew)\n",
    "\n",
    "    \"\"\"\n",
    "        End of for loop\n",
    "        ---------------\n",
    "        Get last Values for BOOTSTRAPING\n",
    "    \"\"\"\n",
    "    logits, value = model.call(np.expand_dims(obs, axis=0))\n",
    "    action_t = tf.squeeze(tf.random.categorical(logits, 1), axis=-1)\n",
    "    last_values = value\n",
    "\n",
    "    vec_obses = np.array(vec_obses, dtype= np.float32)\n",
    "    rews = np.array(rews, dtype=np.float32).flatten()\n",
    "    dones = np.array(dones, dtype= np.bool)\n",
    "    values = np.array(values, dtype=np.float32).flatten()\n",
    "    logps = np.array(logps, dtype=np.float32).flatten()\n",
    "    actions = np.array(actions, dtype=np.int32).flatten()\n",
    "\n",
    "    \"\"\"\n",
    "        Discount / Bootstrap Values and calc Advantages\n",
    "        -----------------------------------------------\n",
    "    \"\"\"\n",
    "    returns = np.zeros_like(rews)\n",
    "    advs = np.zeros_like(rews)\n",
    "    last_gae_lam = 0\n",
    "\n",
    "    for t in reversed(range(num_steps)):\n",
    "        if t == num_steps - 1:\n",
    "            next_non_terminal = 1.0 - done_t\n",
    "            next_values = last_values\n",
    "        else:\n",
    "            next_non_terminal = 1.0 - dones[t + 1]\n",
    "            next_values = values[t + 1]\n",
    "\n",
    "        delta = rews[t] + gamma * next_values * next_non_terminal - values[t]\n",
    "        advs[t] = last_gae_lam = delta + gamma * lam * next_non_terminal * last_gae_lam\n",
    "\n",
    "    returns = advs + values\n",
    "    advs = (advs - advs.mean()) / (advs.std())\n",
    "#     print(returns)\n",
    "    m_losses = update(model, vec_obses, actions, advs, returns, logps)\n",
    "    print(m_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "august-graphics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pi_loss': -0.021982225, 'v_loss': 98.77708, 'entropy_loss': -0.30000085, 'approx_ent': 0.26633313, 'approx_kl': 0.022213459}\n"
     ]
    }
   ],
   "source": [
    "print(m_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-raising",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
